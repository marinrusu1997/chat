name: chatapp-cluster

networks:
  etcd_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.15.0.0/16
  postgres_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.18.0.0/16
  scylla_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.19.0.0/16
  monitoring_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
  chat_app_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
  redis_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/16
  elasticsearch_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.23.0.0/16
  neo4j_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.24.0.0/16
  kafka_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16
  nats_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.26.0.0/16
  mailpit_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.27.0.0/16

volumes:
  etcd-data-1:
  etcd-data-2:
  etcd-data-3:
  pg-data-1:
  pg-data-2:
  pg-data-3:
  scylla-data-1:
  scylla-data-2:
  scylla-data-3:
  scylla-setup-data:
  redis-data-1:
  redis-data-2:
  redis-data-3:
  redis-data-4:
  redis-data-5:
  redis-data-6:
  es-setup-data-shared:
  es-setup-data-internal:
  es-master-data-1:
  es-hot-data-1:
  es-warm-data-1:
  es-kibana-data:
  es-filebeat-data:
  neo4j-data-1:
  kafka-data-1:
  kafka-data-2:
  kafka-data-3:
  nats-data-1:
  nats-data-2:
  nats-data-3:
  mailpit-data:
  prometheus-data:
  grafana-data:

x-logging: &default-logging
  driver: local
  options:
    max-size: "5m"
    max-file: "2"
    compress: "true"

x-etcd-dependencies: &etcd-dependencies
  etcd-node-1:
    condition: service_healthy
  etcd-node-2:
    condition: service_healthy
  etcd-node-3:
    condition: service_healthy

x-postgres-dependencies: &postgres-dependencies
  pg-node-1:
    condition: service_healthy
  pg-node-2:
    condition: service_healthy
  pg-node-3:
    condition: service_healthy

x-scylla-dependencies: &scylla-dependencies
  scylla-node-1:
    condition: service_healthy
  scylla-node-2:
    condition: service_healthy
  scylla-node-3:
    condition: service_healthy

x-redis-dependencies: &redis-dependencies
  redis-node-1:
    condition: service_healthy
  redis-node-2:
    condition: service_healthy
  redis-node-3:
    condition: service_healthy
  redis-node-4:
    condition: service_healthy
  redis-node-5:
    condition: service_healthy
  redis-node-6:
    condition: service_healthy

x-elasticsearch-dependencies: &elasticsearch-dependencies
  es-master-1:
    condition: service_healthy
  es-data-hot-1:
    condition: service_healthy
  es-data-warm-1:
    condition: service_healthy
  es-coordinating-1:
    condition: service_healthy

x-kafka-dependencies: &kafka-dependencies
  kafka1:
    condition: service_healthy
  kafka2:
    condition: service_healthy
  kafka3:
    condition: service_healthy

x-nats-dependencies: &nats-dependencies
  nats-node-1:
    condition: service_healthy
  nats-node-2:
    condition: service_healthy
  nats-node-3:
    condition: service_healthy

services:
  etcd-node-1:
    image: quay.io/coreos/etcd:v3.6.5
    container_name: etcd-node-1
    hostname: etcd-node-1
    logging: *default-logging
    networks:
      - etcd_backend_network
    volumes:
      - etcd-data-1:/var/log/etcd
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/etcd/certs/ca/public.crt:ro
      - ./cert-generator/output/services/etcd-node-1/private.key:/etc/etcd/certs/private.key:ro
      - ./cert-generator/output/services/etcd-node-1/public.crt:/etc/etcd/certs/public.crt:ro
    command: >
      /usr/local/bin/etcd
      --name etcd-node-1
      --data-dir /var/log/etcd
      --initial-advertise-peer-urls https://etcd-node-1:2380
      --listen-peer-urls https://0.0.0.0:2380
      --listen-client-urls https://0.0.0.0:2379
      --advertise-client-urls https://etcd-node-1:2379
      --initial-cluster etcd-node-1=https://etcd-node-1:2380,etcd-node-2=https://etcd-node-2:2380,etcd-node-3=https://etcd-node-3:2380
      --initial-cluster-token etcd-cluster-1
      --initial-cluster-state new
      --client-cert-auth
      --trusted-ca-file=/etc/etcd/certs/ca/public.crt
      --cert-file=/etc/etcd/certs/public.crt
      --key-file=/etc/etcd/certs/private.key
      --peer-client-cert-auth
      --peer-trusted-ca-file=/etc/etcd/certs/ca/public.crt
      --peer-cert-file=/etc/etcd/certs/public.crt
      --peer-key-file=/etc/etcd/certs/private.key
      --log-level info
      --logger zap
      --log-outputs stderr
      --heartbeat-interval=100
      --election-timeout=1000
      --snapshot-count=200000
      --quota-backend-bytes=8589934592
      --max-request-bytes=1572864
      --max-txn-ops=128
      --auto-compaction-mode=periodic
      --auto-compaction-retention=24h
      --grpc-keepalive-min-time=10s
      --grpc-keepalive-interval=10s
      --grpc-keepalive-timeout=10s
    ulimits:
      nofile: 262144
      nproc: 65535
    healthcheck:
      test: [
        "CMD",
        "/usr/local/bin/etcdctl",
        "--endpoints=https://etcd-node-1:2379",
        "--cacert=/etc/etcd/certs/ca/public.crt",
        "--cert=/etc/etcd/certs/public.crt",
        "--key=/etc/etcd/certs/private.key",
        "endpoint",
        "health"
      ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1G
    cpus: 2

  etcd-node-2:
    image: quay.io/coreos/etcd:v3.6.5
    container_name: etcd-node-2
    hostname: etcd-node-2
    logging: *default-logging
    networks:
      - etcd_backend_network
    volumes:
      - etcd-data-2:/var/log/etcd
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/etcd/certs/ca/public.crt:ro
      - ./cert-generator/output/services/etcd-node-2/private.key:/etc/etcd/certs/private.key:ro
      - ./cert-generator/output/services/etcd-node-2/public.crt:/etc/etcd/certs/public.crt:ro
    command: >
      /usr/local/bin/etcd
      --name etcd-node-2
      --data-dir /var/log/etcd
      --initial-advertise-peer-urls https://etcd-node-2:2380
      --listen-peer-urls https://0.0.0.0:2380
      --listen-client-urls https://0.0.0.0:2379
      --advertise-client-urls https://etcd-node-2:2379
      --initial-cluster etcd-node-1=https://etcd-node-1:2380,etcd-node-2=https://etcd-node-2:2380,etcd-node-3=https://etcd-node-3:2380
      --initial-cluster-token etcd-cluster-1
      --initial-cluster-state new
      --client-cert-auth
      --trusted-ca-file=/etc/etcd/certs/ca/public.crt
      --cert-file=/etc/etcd/certs/public.crt
      --key-file=/etc/etcd/certs/private.key
      --peer-client-cert-auth
      --peer-trusted-ca-file=/etc/etcd/certs/ca/public.crt
      --peer-cert-file=/etc/etcd/certs/public.crt
      --peer-key-file=/etc/etcd/certs/private.key
      --log-level info
      --logger zap
      --log-outputs stderr
      --heartbeat-interval=100
      --election-timeout=1000
      --snapshot-count=200000
      --quota-backend-bytes=8589934592
      --max-request-bytes=1572864
      --max-txn-ops=128
      --auto-compaction-mode=periodic
      --auto-compaction-retention=24h
      --grpc-keepalive-min-time=10s
      --grpc-keepalive-interval=10s
      --grpc-keepalive-timeout=10s
    ulimits:
      nofile: 262144
      nproc: 65535
    healthcheck:
      test: [
        "CMD",
        "/usr/local/bin/etcdctl",
        "--endpoints=https://etcd-node-2:2379",
        "--cacert=/etc/etcd/certs/ca/public.crt",
        "--cert=/etc/etcd/certs/public.crt",
        "--key=/etc/etcd/certs/private.key",
        "endpoint",
        "health"
      ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1G
    cpus: 2

  etcd-node-3:
    image: quay.io/coreos/etcd:v3.6.5
    container_name: etcd-node-3
    hostname: etcd-node-3
    logging: *default-logging
    networks:
      - etcd_backend_network
    volumes:
      - etcd-data-3:/var/log/etcd
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/etcd/certs/ca/public.crt:ro
      - ./cert-generator/output/services/etcd-node-3/private.key:/etc/etcd/certs/private.key:ro
      - ./cert-generator/output/services/etcd-node-3/public.crt:/etc/etcd/certs/public.crt:ro
    command: >
      /usr/local/bin/etcd
      --name etcd-node-3
      --data-dir /var/log/etcd
      --initial-advertise-peer-urls https://etcd-node-3:2380
      --listen-peer-urls https://0.0.0.0:2380
      --listen-client-urls https://0.0.0.0:2379
      --advertise-client-urls https://etcd-node-3:2379
      --initial-cluster etcd-node-1=https://etcd-node-1:2380,etcd-node-2=https://etcd-node-2:2380,etcd-node-3=https://etcd-node-3:2380
      --initial-cluster-token etcd-cluster-1
      --initial-cluster-state new
      --client-cert-auth
      --trusted-ca-file=/etc/etcd/certs/ca/public.crt
      --cert-file=/etc/etcd/certs/public.crt
      --key-file=/etc/etcd/certs/private.key
      --peer-client-cert-auth
      --peer-trusted-ca-file=/etc/etcd/certs/ca/public.crt
      --peer-cert-file=/etc/etcd/certs/public.crt
      --peer-key-file=/etc/etcd/certs/private.key
      --log-level info
      --logger zap
      --log-outputs stderr
      --heartbeat-interval=100
      --election-timeout=1000
      --snapshot-count=200000
      --quota-backend-bytes=8589934592
      --max-request-bytes=1572864
      --max-txn-ops=128
      --auto-compaction-mode=periodic
      --auto-compaction-retention=24h
      --grpc-keepalive-min-time=10s
      --grpc-keepalive-interval=10s
      --grpc-keepalive-timeout=10s
    ulimits:
      nofile: 262144
      nproc: 65535
    healthcheck:
      test: [
        "CMD",
        "/usr/local/bin/etcdctl",
        "--endpoints=https://etcd-node-3:2379",
        "--cacert=/etc/etcd/certs/ca/public.crt",
        "--cert=/etc/etcd/certs/public.crt",
        "--key=/etc/etcd/certs/private.key",
        "endpoint",
        "health"
      ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1G
    cpus: 2

  pg-node-1:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-1
    hostname: pg-node-1
    logging: *default-logging
    networks:
      postgres_backend_network:
        aliases:
          - pg-node-1-internal # Give this node a specific name on the backend network
      etcd_backend_network:
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-1:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/patroni/certs/ca/public.crt:ro
      - ./cert-generator/output/services/pg-node-1/private.key:/etc/patroni/certs/private.key:ro
      - ./cert-generator/output/services/pg-node-1/public.crt:/etc/patroni/certs/public.crt:ro
    environment:
      - PATRONI_NAME=pg-node-1
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-1:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-1-internal:5432
      - PGPOOL_BACKEND_NODE_ID=0
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q"]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12
    depends_on: *etcd-dependencies

  pg-node-2:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-2
    hostname: pg-node-2
    logging: *default-logging
    networks:
      postgres_backend_network:
        aliases:
          - pg-node-2-internal # Give this node a specific name on the backend network
      etcd_backend_network:
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-2:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/patroni/certs/ca/public.crt:ro
      - ./cert-generator/output/services/pg-node-2/private.key:/etc/patroni/certs/private.key:ro
      - ./cert-generator/output/services/pg-node-2/public.crt:/etc/patroni/certs/public.crt:ro
    environment:
      - PATRONI_NAME=pg-node-2
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-2:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-2-internal:5432
      - PGPOOL_BACKEND_NODE_ID=1
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: [ "CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q" ]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12
    depends_on: *etcd-dependencies

  pg-node-3:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-3
    hostname: pg-node-3
    logging: *default-logging
    networks:
      postgres_backend_network:
        aliases:
          - pg-node-3-internal # Give this node a specific name on the backend network
      etcd_backend_network:
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-3:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/patroni/certs/ca/public.crt:ro
      - ./cert-generator/output/services/pg-node-3/private.key:/etc/patroni/certs/private.key:ro
      - ./cert-generator/output/services/pg-node-3/public.crt:/etc/patroni/certs/public.crt:ro
    environment:
      - PATRONI_NAME=pg-node-3
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-3:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-3-internal:5432
      - PGPOOL_BACKEND_NODE_ID=2
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: [ "CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q" ]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12
    depends_on: *etcd-dependencies

  pgpool-node-1:
    build:
      context: ./pgpool
      dockerfile: Dockerfile
    container_name: pgpool-node-1
    hostname: pgpool-node-1
    logging: *default-logging
    networks:
      - postgres_backend_network
    volumes:
      # Configuration
      - ./pgpool/conf/pcp.conf:/etc/pgpool2/pcp.conf
      - ./pgpool/conf/pgpool.conf:/tmp/pgpool.conf.template:ro
      - ./pgpool/conf/pool_hba.conf:/etc/pgpool2/pool_hba.conf:ro
      # SSL Certificates
      - ./cert-generator/output/services/pgpool-node-1/private.key:/etc/pgpool2/certs/private.key:ro
      - ./cert-generator/output/services/pgpool-node-1/public.crt:/etc/pgpool2/certs/public.crt:ro
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./pgpool/secrets/pgpoolkey.txt:/var/lib/postgresql/.pgpoolkey
      - ./pgpool/secrets/pcppass.txt:/var/lib/postgresql/.pcppass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass:ro
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_external_user_chat_ro.pass:ro
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_external_user_chat_rw.pass:ro
      # Scripts
      - ./pgpool/scripts/pre_init.sh:/etc/pgpool2/custom_pre_init.sh:ro
      - ./pgpool/scripts/follow_primary.sh:/etc/pgpool2/custom_follow_primary.sh:ro
      - ./pgpool/scripts/failover.sh:/etc/pgpool2/custom_failover.sh:ro
      - ./pgpool/scripts/failback.sh:/etc/pgpool2/custom_failback.sh:ro
    command: >
      sh -c "
        chown postgres:postgres /var/lib/postgresql/.pgpoolkey && chmod 400 /var/lib/postgresql/.pgpoolkey &&
        chown postgres:postgres /etc/pgpool2/pcp.conf && chmod 400 /etc/pgpool2/pcp.conf &&
        bash /etc/pgpool2/custom_pre_init.sh &&
        chown postgres:postgres /var/lib/postgresql/.pcppass && chmod 400 /var/lib/postgresql/.pcppass && 
        rm -f /tmp/pgpool.pid /tmp/.s.PGSQL.* && 
        exec gosu postgres /usr/sbin/pgpool -n -f /etc/pgpool2/pgpool.conf
      "
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_external_user_chat_ro.pass gosu postgres pg_isready -h pgpool-node-1 -p 9999 -U chat_ro -d chat_db -q"]
      interval: 60s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1G
    cpus: 4
    depends_on: *postgres-dependencies

  pg-exporter:
    build:
      context: ./pg-exporter
      dockerfile: Dockerfile
    container_name: pg-exporter
    hostname: pg-exporter
    logging: *default-logging
    networks:
      - postgres_backend_network
      - monitoring_network
    volumes:
      - ./pg-exporter/postgres_exporter.yml:/tmp/postgres_exporter.yml.template:ro
      - ./pg-exporter/entrypoint.sh:/tmp/entrypoint.sh:ro
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass:ro
    entrypoint: [ '/tmp/entrypoint.sh' ]
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:9187/" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 200MB
    cpus: 1
    depends_on:
      pgpool-node-1:
        condition: service_healthy

  scylla-node-1:
    image: scylladb/scylla:2025.3.3
    container_name: scylla-node-1
    hostname: scylla-node-1
    logging: *default-logging
    networks:
      - scylla_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/scylla/certs/ca/public.crt:ro
      - ./cert-generator/output/services/scylla-node-1/private.key:/etc/scylla/certs/private.key:ro
      - ./cert-generator/output/services/scylla-node-1/public.crt:/etc/scylla/certs/public.crt:ro
      # Config
      - ./scylla/config/rack1.properties:/etc/scylla/cassandra-rackdc.properties:ro
      # Scripts
      - ./scylla/scripts/healthcheck.sh:/usr/local/bin/scylla/healthcheck.sh:ro
      # Data
      - scylla-data-1:/var/lib/scylla
    command: >
      --seeds=scylla-node-1
      --listen-address=scylla-node-1
      --rpc-address=scylla-node-1
      --storage-port=7000
      --ssl-storage-port=7001
      --native-transport-port=9042
      --native-shard-aware-transport-port=19042
      --native-transport-port-ssl=9142
      --native-shard-aware-transport-port-ssl=19142
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
      --server-encryption-options=internode_encryption=all
      --server-encryption-options=certificate=/etc/scylla/certs/public.crt
      --server-encryption-options=keyfile=/etc/scylla/certs/private.key
      --server-encryption-options=truststore=/etc/scylla/certs/ca/public.crt
      --server-encryption-options=priority_string=SECURE128:-VERS-TLS1.0:-VERS-TLS1.1:-VERS-TLS1.2
      --server-encryption-options=require_client_auth=true
      --client-encryption-options=enabled=true
      --client-encryption-options=certificate=/etc/scylla/certs/public.crt
      --client-encryption-options=keyfile=/etc/scylla/certs/private.key
      --client-encryption-options=truststore=/etc/scylla/certs/ca/public.crt
      --client-encryption-options=priority_string=SECURE128:-VERS-TLS1.0:-VERS-TLS1.1:-VERS-TLS1.2
      --client-encryption-options=require_client_auth=true
      --client-encryption-options=enable_session_tickets=true
    healthcheck:
      test: [ "CMD-SHELL", "/bin/bash /usr/local/bin/scylla/healthcheck.sh && sleep 10" ]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN
    security_opt:
      - "seccomp:unconfined"

  scylla-node-2:
    image: scylladb/scylla:2025.3.3
    container_name: scylla-node-2
    hostname: scylla-node-2
    logging: *default-logging
    networks:
      - scylla_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/scylla/certs/ca/public.crt:ro
      - ./cert-generator/output/services/scylla-node-2/private.key:/etc/scylla/certs/private.key:ro
      - ./cert-generator/output/services/scylla-node-2/public.crt:/etc/scylla/certs/public.crt:ro
      # Config
      - ./scylla/config/rack2.properties:/etc/scylla/cassandra-rackdc.properties:ro
      # Scripts
      - ./scylla/scripts/healthcheck.sh:/usr/local/bin/scylla/healthcheck.sh:ro
      # Data
      - scylla-data-2:/var/lib/scylla
    command: >
      --seeds=scylla-node-1
      --listen-address=scylla-node-2
      --rpc-address=scylla-node-2
      --storage-port=7000
      --ssl-storage-port=7001
      --native-transport-port=9042
      --native-shard-aware-transport-port=19042
      --native-transport-port-ssl=9142
      --native-shard-aware-transport-port-ssl=19142
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
      --server-encryption-options=internode_encryption=all
      --server-encryption-options=certificate=/etc/scylla/certs/public.crt
      --server-encryption-options=keyfile=/etc/scylla/certs/private.key
      --server-encryption-options=truststore=/etc/scylla/certs/ca/public.crt
      --server-encryption-options=priority_string=SECURE128:-VERS-TLS1.0:-VERS-TLS1.1:-VERS-TLS1.2
      --server-encryption-options=require_client_auth=true
      --client-encryption-options=enabled=true
      --client-encryption-options=certificate=/etc/scylla/certs/public.crt
      --client-encryption-options=keyfile=/etc/scylla/certs/private.key
      --client-encryption-options=truststore=/etc/scylla/certs/ca/public.crt
      --client-encryption-options=priority_string=SECURE128:-VERS-TLS1.0:-VERS-TLS1.1:-VERS-TLS1.2
      --client-encryption-options=require_client_auth=true
      --client-encryption-options=enable_session_tickets=true
    healthcheck:
      test: [ "CMD-SHELL", "/bin/bash /usr/local/bin/scylla/healthcheck.sh && sleep 5" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN
    security_opt:
      - "seccomp:unconfined"
    depends_on:
      scylla-node-1:
        condition: service_healthy

  scylla-node-3:
    image: scylladb/scylla:2025.3.3
    container_name: scylla-node-3
    hostname: scylla-node-3
    logging: *default-logging
    networks:
      - scylla_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/scylla/certs/ca/public.crt:ro
      - ./cert-generator/output/services/scylla-node-3/private.key:/etc/scylla/certs/private.key:ro
      - ./cert-generator/output/services/scylla-node-3/public.crt:/etc/scylla/certs/public.crt:ro
      # Config
      - ./scylla/config/rack3.properties:/etc/scylla/cassandra-rackdc.properties:ro
      # Scripts
      - ./scylla/scripts/healthcheck.sh:/usr/local/bin/scylla/healthcheck.sh:ro
      # Data
      - scylla-data-3:/var/lib/scylla
    command: >
      --seeds=scylla-node-1
      --listen-address=scylla-node-3
      --rpc-address=scylla-node-3
      --storage-port=7000
      --ssl-storage-port=7001
      --native-transport-port=9042
      --native-shard-aware-transport-port=19042
      --native-transport-port-ssl=9142
      --native-shard-aware-transport-port-ssl=19142
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
      --server-encryption-options=internode_encryption=all
      --server-encryption-options=certificate=/etc/scylla/certs/public.crt
      --server-encryption-options=keyfile=/etc/scylla/certs/private.key
      --server-encryption-options=truststore=/etc/scylla/certs/ca/public.crt
      --server-encryption-options=priority_string=SECURE128:-VERS-TLS1.0:-VERS-TLS1.1:-VERS-TLS1.2
      --server-encryption-options=require_client_auth=true
      --client-encryption-options=enabled=true
      --client-encryption-options=certificate=/etc/scylla/certs/public.crt
      --client-encryption-options=keyfile=/etc/scylla/certs/private.key
      --client-encryption-options=truststore=/etc/scylla/certs/ca/public.crt
      --client-encryption-options=priority_string=SECURE128:-VERS-TLS1.0:-VERS-TLS1.1:-VERS-TLS1.2
      --client-encryption-options=require_client_auth=true
      --client-encryption-options=enable_session_tickets=true
    healthcheck:
      test: [ "CMD-SHELL", "/bin/bash /usr/local/bin/scylla/healthcheck.sh && sleep 5" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN
    security_opt:
      - "seccomp:unconfined"
    depends_on:
      scylla-node-2:
        condition: service_healthy

  scylla-setup:
    image: scylladb/scylla:2025.3.3
    container_name: scylla-setup
    hostname: scylla-setup
    logging: *default-logging
    networks:
      - scylla_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/scylla/certs/ca/public.crt:ro
      - ./cert-generator/output/services/scylla-setup/private.key:/etc/scylla/certs/private.key:ro
      - ./cert-generator/output/services/scylla-setup/public.crt:/etc/scylla/certs/public.crt:ro
      # Scripts
      - ./scylla/scripts/init.cql:/usr/local/bin/scylla/init.cql:ro
      - ./scylla/scripts/init.sh:/usr/local/bin/scylla/init.sh:ro
      # Config
      - ./scylla/config/cqlshrc:/etc/scylla/client/cqlshrc:ro
      # Data
      - scylla-setup-data:/var/lib/scylla
    env_file:
      - ./scylla/secrets/.env.secrets
    entrypoint: [ "/bin/bash", "/usr/local/bin/scylla/init.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "[ -f /var/lib/scylla/locks/initialization.lock ]" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: no
    mem_limit: 200M
    cpus: 1
    depends_on: *scylla-dependencies

  redis-node-1:
    image: redis:8.2.1
    container_name: redis-node-1
    hostname: redis-node-1
    logging: *default-logging
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-1/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-1/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-1:/data
    entrypoint: /usr/local/etc/redis/scripts/entrypoint.sh
    command: >
      /usr/local/etc/redis/redis.conf 
      --cluster-announce-hostname redis-node-1
      --cluster-announce-human-nodename redis-node-1
      --cluster-announce-ip redis-node-1
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-2:
    image: redis:8.2.1
    container_name: redis-node-2
    hostname: redis-node-2
    logging: *default-logging
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-2/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-2/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-2:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-2
      --cluster-announce-human-nodename redis-node-2
      --cluster-announce-ip redis-node-2
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-3:
    image: redis:8.2.1
    container_name: redis-node-3
    hostname: redis-node-3
    logging: *default-logging
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-3/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-3/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-3:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-3
      --cluster-announce-human-nodename redis-node-3
      --cluster-announce-ip redis-node-3
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-4:
    image: redis:8.2.1
    container_name: redis-node-4
    hostname: redis-node-4
    logging: *default-logging
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-4/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-4/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-4:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-4
      --cluster-announce-human-nodename redis-node-4
      --cluster-announce-ip redis-node-4
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-5:
    image: redis:8.2.1
    container_name: redis-node-5
    hostname: redis-node-5
    logging: *default-logging
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-5/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-5/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-5:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-5
      --cluster-announce-human-nodename redis-node-5
      --cluster-announce-ip redis-node-5
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-6:
    image: redis:8.2.1
    container_name: redis-node-6
    hostname: redis-node-6
    logging: *default-logging
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-6/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-6/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-6:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-6
      --cluster-announce-human-nodename redis-node-6
      --cluster-announce-ip redis-node-6
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  es-setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-setup
    hostname: es-setup
    logging: *default-logging
    networks:
      - elasticsearch_backend_network
    volumes:
      # Named volumes
      - es-setup-data-shared:/usr/share/elasticsearch/config/certs
      - es-setup-data-internal:/var/lib/elasticsearch
      # Config
      - ./elasticsearch/config/instances.yml:/etc/elasticsearch/instances.yml:ro
      - ./elasticsearch/schemas:/etc/elasticsearch/schemas:ro
      # Scripts
      - ./elasticsearch/scripts/init.sh:/etc/elasticsearch/scripts/init.sh:ro
      - ./redis/scripts/logger.sh:/etc/elasticsearch/scripts/logger.sh:ro
    environment:
      - ELASTICSEARCH_ELASTIC_USERNAME=${ELASTICSEARCH_ELASTIC_USERNAME}
      - ELASTICSEARCH_ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - ELASTICSEARCH_KIBANA_SYSTEM_USERNAME=${ELASTICSEARCH_KIBANA_SYSTEM_USERNAME}
      - ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD=${ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD}
      - ELASTICSEARCH_KIBANA_ADMIN_USERNAME=${ELASTICSEARCH_KIBANA_ADMIN_USERNAME}
      - ELASTICSEARCH_KIBANA_ADMIN_PASSWORD=${ELASTICSEARCH_KIBANA_ADMIN_PASSWORD}
      - ELASTICSEARCH_CHAT_APP_USERNAME=${ELASTICSEARCH_CHAT_APP_USERNAME}
      - ELASTICSEARCH_CHAT_APP_PASSWORD=${ELASTICSEARCH_CHAT_APP_PASSWORD}
      - ELASTICSEARCH_FILEBEAT_WRITER_USERNAME=${ELASTICSEARCH_FILEBEAT_WRITER_USERNAME}
      - ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD=${ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD}
      - ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME=${ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME}
      - ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD=${ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD}
    entrypoint: [ "/bin/bash", "/etc/elasticsearch/scripts/init.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "[ -f /usr/share/elasticsearch/config/certs/es-master-1/es-master-1.crt ]" ]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: no
    mem_limit: 200M
    cpus: 1
    user: "0"

  es-master-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-master-1
    hostname: es-master-1
    logging: *default-logging
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.11
    volumes:
      - es-setup-data-shared:/usr/share/elasticsearch/config/certs:ro
      - es-master-data-1:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.11
      - node.name=es-master-1
      - node.roles=master
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=""
      - cluster.initial_master_nodes=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true # Rule: Disable swapping for performance
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" # Rule: Set -Xms and -Xmx equal. Use small heap for master nodes.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=/usr/share/elasticsearch/config/certs/es-master-1/es-master-1.key
      - xpack.security.http.ssl.certificate=/usr/share/elasticsearch/config/certs/es-master-1/es-master-1.crt
      - xpack.security.http.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=/usr/share/elasticsearch/config/certs/es-master-1/es-master-1.key
      - xpack.security.transport.ssl.certificate=/usr/share/elasticsearch/config/certs/es-master-1/es-master-1.crt
      - xpack.security.transport.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL","curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-master-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 2GB
    cpus: 1
    depends_on:
      es-setup: { condition: service_healthy }

  es-data-hot-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-data-hot-1
    hostname: es-data-hot-1
    logging: *default-logging
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.21
    volumes:
      - es-setup-data-shared:/usr/share/elasticsearch/config/certs:ro
      - es-hot-data-1:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.21
      - node.name=es-data-hot-1
      - node.roles=data
      - node.attr.data_tier=hot
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g" # IMPORTANT: Set this to <= 50% of the host RAM, but do not exceed ~31GB.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=/usr/share/elasticsearch/config/certs/es-data-hot-1/es-data-hot-1.key
      - xpack.security.http.ssl.certificate=/usr/share/elasticsearch/config/certs/es-data-hot-1/es-data-hot-1.crt
      - xpack.security.http.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=/usr/share/elasticsearch/config/certs/es-data-hot-1/es-data-hot-1.key
      - xpack.security.transport.ssl.certificate=/usr/share/elasticsearch/config/certs/es-data-hot-1/es-data-hot-1.crt
      - xpack.security.transport.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-data-hot-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 6GB
    cpus: 6
    depends_on:
      es-setup: { condition: service_healthy }

  es-data-warm-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-data-warm-1
    hostname: es-data-warm-1
    logging: *default-logging
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.22
    volumes:
      - es-setup-data-shared:/usr/share/elasticsearch/config/certs:ro
      - es-warm-data-1:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.22
      - node.name=es-data-warm-1
      - node.roles=data
      - node.attr.data_tier=warm
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g" # IMPORTANT: Set this to <= 50% of the host RAM, but do not exceed ~31GB.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=/usr/share/elasticsearch/config/certs/es-data-warm-1/es-data-warm-1.key
      - xpack.security.http.ssl.certificate=/usr/share/elasticsearch/config/certs/es-data-warm-1/es-data-warm-1.crt
      - xpack.security.http.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=/usr/share/elasticsearch/config/certs/es-data-warm-1/es-data-warm-1.key
      - xpack.security.transport.ssl.certificate=/usr/share/elasticsearch/config/certs/es-data-warm-1/es-data-warm-1.crt
      - xpack.security.transport.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-data-warm-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 6GB
    cpus: 6
    depends_on:
      es-setup: { condition: service_healthy }

  es-coordinating-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-coordinating-1
    hostname: es-coordinating-1
    logging: *default-logging
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.31
    volumes:
      - es-setup-data-shared:/usr/share/elasticsearch/config/certs:ro
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.31
      - node.name=es-coordinating-1
      - node.roles="" # Empty role makes it coordinating-only
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms800m -Xmx800m" # Moderate heap for query aggregation.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=/usr/share/elasticsearch/config/certs/es-coordinating-1/es-coordinating-1.key
      - xpack.security.http.ssl.certificate=/usr/share/elasticsearch/config/certs/es-coordinating-1/es-coordinating-1.crt
      - xpack.security.http.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=/usr/share/elasticsearch/config/certs/es-coordinating-1/es-coordinating-1.key
      - xpack.security.transport.ssl.certificate=/usr/share/elasticsearch/config/certs/es-coordinating-1/es-coordinating-1.crt
      - xpack.security.transport.ssl.certificate_authorities=/usr/share/elasticsearch/config/certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-coordinating-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 4GB
    cpus: 4
    depends_on:
      es-setup: { condition: service_healthy }

  es-kibana:
    image: docker.elastic.co/kibana/kibana:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-kibana
    hostname: es-kibana
    logging: *default-logging
    ports:
      - "5781:5601"
    networks:
      - elasticsearch_backend_network
    volumes:
      - type: volume
        source: es-setup-data-shared
        target: /usr/share/elasticsearch/config/certs
        read_only: true
        volume:
          subpath: ca
      - es-kibana-data:/usr/share/kibana/data
    environment:
      - SERVERNAME=es-kibana
      - ELASTICSEARCH_HOSTS=https://es-coordinating-1:9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_KIBANA_SYSTEM_USERNAME}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=/usr/share/elasticsearch/config/certs/ca.crt
    healthcheck:
      test: [ "CMD-SHELL", "curl -s --fail http://localhost:5601/api/status || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1GB
    cpus: 2
    depends_on: *elasticsearch-dependencies

  es-filebeat:
    image: docker.elastic.co/beats/filebeat-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-filebeat
    hostname: es-filebeat
    user: root # Required to access Docker log files and socket
    logging: *default-logging
    networks:
      - elasticsearch_backend_network
    volumes:
      # Scripts
      - ./filebeat/scripts/entrypoint.sh:/usr/local/bin/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/usr/local/bin/logger.sh:ro
      # Mounts for collecting logs and metadata from the Docker daemon
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      # Mount shared certificates for secure connection to Elasticsearch
      - type: volume
        source: es-setup-data-shared
        target: /usr/share/elasticsearch/config/certs
        read_only: true
        volume:
          subpath: ca
      # Mount the configuration file
      - ./filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml
      # Persist Filebeat's registry data to track log file positions
      - es-filebeat-data:/usr/share/filebeat/data
    environment:
      # --- Credentials for the 'setup' command ---
      - KIBANA_HOST=es-kibana:5601
      - ELASTICSEARCH_ELASTIC_USERNAME=${ELASTICSEARCH_ELASTIC_USERNAME}
      - ELASTICSEARCH_ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      # Credentials for sending logs to Elasticsearch
      - ELASTICSEARCH_HOSTS=https://es-coordinating-1:9200
      - ELASTICSEARCH_FILEBEAT_WRITER_USERNAME=${ELASTICSEARCH_FILEBEAT_WRITER_USERNAME}
      - ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD=${ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD}
      # Credentials for sending Filebeat's own monitoring data
      - ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME=${ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME}
      - ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD=${ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD}
    entrypoint: ["/usr/local/bin/entrypoint.sh"]
    healthcheck:
      test: [ "CMD-SHELL", "filebeat test output" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 1GB
    cpus: 2
    depends_on:
      es-kibana:
        condition: service_healthy

  neo4j-node-1:
    image: neo4j:5.26.12-enterprise-ubi9
    container_name: neo4j-node-1
    hostname: neo4j-node-1
    logging: *default-logging
    networks:
      - neo4j_backend_network
    ports:
      - "7473:7473"   # HTTPS
    volumes:
      # Data
      - neo4j-data-1:/data
      # Certificates
      ## Bolt
      - ./cert-generator/output/ca/root.crt:/tmp/certificates/bolt/trusted/public.crt:ro
      - ./cert-generator/output/services/neo4j-node-1/private.key:/tmp/certificates/bolt/private.key:ro
      - ./cert-generator/output/services/neo4j-node-1/public.crt:/tmp/certificates/bolt/public.crt:ro
      ## Https
      - ./cert-generator/output/ca/root.crt:/tmp/certificates/https/trusted/public.crt:ro
      - ./cert-generator/output/services/neo4j-node-1/private.key:/tmp/certificates/https/private.key:ro
      - ./cert-generator/output/services/neo4j-node-1/public.crt:/tmp/certificates/https/public.crt:ro
      # Scripts
      - ./neo4j/scripts/setup.sh:/startup/setup.sh:ro
      - ./neo4j/scripts/entrypoint.sh:/startup/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/startup/logger.sh:ro
      # Schemas
      - ./neo4j/schema/chat.cypher:/schema/chat.cypher:ro
    environment:
      # --- Authentication & License ---
      # This password is used by the setup script and should be changed or secured.
      - NEO4J_AUTH=neo4j/${NEO4J_NEO4J_USER_PASSWORD}
      - SCRIPT_PASSWORD_NEO4J=${NEO4J_NEO4J_USER_PASSWORD}
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes

      # --- Performance Tuning: Memory ---
      - NEO4J_server_memory_heap_initial__size=1G
      - NEO4J_server_memory_heap_max__size=1G
      - NEO4J_server_memory_pagecache_size=2.5G

      # --- Performance Tuning: Checkpointing & Transactions ---
      - NEO4J_db_checkpoint=VOLUMETRIC # Sets checkpointing policy based on transaction log volume, ideal for write-heavy loads.
      - NEO4J_db_lock_acquisition_timeout=10s # Sets a timeout for transactions waiting to acquire a lock, preventing deadlocks.
      - NEO4J_db_tx__log_rotation_retention__policy=2 days 2G # Specifies how long to keep transaction logs.

      # --- Security and Networking ---
      - NEO4J_server_default__listen__address=0.0.0.0
      - NEO4J_dbms_netty_ssl_provider=OPENSSL
      - NEO4J_server_bolt_enabled=true
      - NEO4J_server_bolt_tls__level=REQUIRED
      - NEO4J_dbms_ssl_policy_bolt_enabled=true
      - NEO4J_dbms_ssl_policy_bolt_base_directory=/var/lib/neo4j/certificates/bolt
      - NEO4J_dbms_ssl_policy_bolt_private_key=private.key
      - NEO4J_dbms_ssl_policy_bolt_public_certificate=public.crt
      - NEO4J_dbms_ssl_policy_bolt_client_auth=OPTIONAL
      - NEO4J_server_http_enabled=false
      - NEO4J_server_https_enabled=true
      - NEO4J_dbms_ssl_policy_https_enabled=true
      - NEO4J_dbms_ssl_policy_https_base_directory=/var/lib/neo4j/certificates/https
      - NEO4J_dbms_ssl_policy_https_private_key=private.key
      - NEO4J_dbms_ssl_policy_https_public_certificate=public.crt
      - NEO4J_dbms_ssl_policy_https_client_auth=NONE

      # --- Monitoring ---
      - NEO4J_server_metrics_prometheus_enabled=false

      # --- Connectivity ---
      - NEO4J_initial_dbms_default__database=chatdb
    entrypoint: [ "/startup/entrypoint.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "cypher-shell -a bolt+ssc://localhost:7687 -u neo4j -p ${NEO4J_NEO4J_USER_PASSWORD} 'RETURN 1'" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    restart: "no"
    mem_limit: 4GB
    cpus: 4

  kafka1:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka1
    hostname: kafka1
    logging: *default-logging
    networks:
      - kafka_backend_network
    volumes:
      # Data
      - kafka-data-1:/var/lib/kafka/data
      # Healthcheck
      - ./kafka/scripts/healthcheck.sh:/opt/kafka/scripts/healthcheck.sh:ro
      - ./kafka/scripts/logger.sh:/opt/kafka/scripts/logger.sh:ro
      - ./kafka/lib/jmxterm-1.0.2-uber.jar:/opt/jmxterm/jmxterm.jar:ro
      - ./kafka/conf/client.properties:/opt/kafka/config/client.properties:ro
      - ./kafka/secrets/kclient.keystore.jks:/opt/kafka/config/certs/kclient.keystore.jks:ro
      # Secrets
      - ./kafka/secrets/kafka.truststore.jks:/opt/kafka/config/certs/kafka.truststore.jks:ro
      - ./kafka/secrets/kafka1.keystore.jks:/opt/kafka/config/certs/kafka.keystore.jks:ro
    environment:
      # --- JMX ---
      KAFKA_JMX_OPTS: >
        -Djava.rmi.server.hostname=kafka1
        -Dcom.sun.management.jmxremote.port=2020
        -Dcom.sun.management.jmxremote.rmi.port=2020
        -Dcom.sun.management.jmxremote.authenticate=true
        -Dcom.sun.management.jmxremote.password.file=/opt/kafka/config/secrets/jmxremote.password
        -Dcom.sun.management.jmxremote.access.file=/opt/kafka/config/secrets/jmxremote.access
        -Dcom.sun.management.jmxremote.ssl=true
        -Dcom.sun.management.jmxremote.ssl.need.client.auth=false
        -Dcom.sun.management.jmxremote.registry.ssl=true
        -Djavax.net.ssl.keyStore=/opt/kafka/config/certs/kafka.keystore.jks
        -Djavax.net.ssl.keyStorePassword=${KAFKA_SSL_KEYSTORE_PASSWORD}
      JMX_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}

      # --- KRaft ---
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:9094,2@kafka2:9094,3@kafka3:9094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 9fnudAZ8ROCSiRomvzgeWg

      # --- LISTENERS ---
      KAFKA_LISTENERS: INTERNAL://:9091,CLIENT://:9092,BROKER://:9093,CONTROLLER://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:9091,CLIENT://kafka1:9092,BROKER://kafka1:9093,CONTROLLER://kafka1:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SSL,CLIENT:SASL_SSL,BROKER:SSL,CONTROLLER:SSL

      # --- BROKER-TO-BROKER & CONTROLLER ---
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # --- SSL / mTLS (listener-specific) ---
      # Broker (used for inter-broker replication) -> enforce mTLS
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_BROKER_SSL_PROTOCOL: TLSv1.3

      # Controller (controller quorum) -> enforce mTLS as well
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_PROTOCOL: TLSv1.3

      # Client listener uses TLS for encryption (clients perform SASL auth over TLS)
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_CLIENT_AUTH: none
      KAFKA_LISTENER_NAME_CLIENT_SSL_PROTOCOL: TLSv1.3

      # Internal listener (for tools, e.g. MirrorMaker2, Kafka Connect) - enforce mTLS
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_INTERNAL_SSL_PROTOCOL: TLSv1.3

      # --- SASL for CLIENT listener ---
      KAFKA_LISTENER_NAME_CLIENT_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENT_PLAIN_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        user_chat_admin="${KAFKA_USER_CHAT_ADMIN_PASSWORD}" \
        user_chat_prod_cons="${KAFKA_USER_CHAT_PROD_CONS_PASSWORD}" \
        user_chat_prod="${KAFKA_USER_CHAT_PRODUCER_PASSWORD}" \
        user_chat_cons="${KAFKA_USER_CHAT_CONSUMER_PASSWORD}" \
        user_schema_registry="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      # --- AUTHORIZATION ---
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_SUPER_USERS: User:kclient@manage;User:kafka1@manage;User:kafka2@manage;User:kafka3@manage
      KAFKA_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=(.*?),\s*OU=(.*?),.*$/$1@$2/L, DEFAULT
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: false

      # --- TOPIC / PERFORMANCE ---
      KAFKA_ELIGIBLE_LEADER_REPLICAS_VERSION: 1
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_CLEANER_THREADS: 3
      KAFKA_LOG_CLEANUP_POLICY: compact
      KAFKA_DELETE_TOPIC_ENABLE: true
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false

      # --- PRODUCER / CONSUMER ---
      KAFKA_MESSAGE_MAX_BYTES: 10485760         # 10 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760   # 10 MB
      KAFKA_FETCH_MAX_BYTES: 104857600          # 100 MB
      KAFKA_MAX_PARTITION_FETCH_BYTES: 10485760 # 10 MB

      # --- DATA RETENTION ---
      KAFKA_LOG_RETENTION_BYTES: 104857600  # 100 MB
      KAFKA_LOG_RETENTION_HOURS: 24         # 1 day
      KAFKA_LOG_ROLL_HOURS: 12              # 0.5 day
      KAFKA_LOG_ROLL_JITTER_HOURS: 1        # 1 hour
      KAFKA_LOG_SEGMENT_BYTES: 15728640     # 15 MB

      # --- LOGGING ---
      KAFKA_LOG4J_LOGGERS: >
        kafka=INFO,
        kafka.request.logger=ERROR,
        kafka.controller=INFO,
        kafka.log.LogCleaner=INFO,
        kafka.authorizer.logger=DEBUG,
        state.change.logger=INFO
    healthcheck:
      test: [ "CMD", "bash", "/opt/kafka/scripts/healthcheck.sh" ]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 4GB
    cpus: 4
    ulimits:
      nofile:
        soft: 100000
        hard: 100000

  kafka2:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka2
    hostname: kafka2
    logging: *default-logging
    networks:
      - kafka_backend_network
    volumes:
      # Data
      - kafka-data-2:/var/lib/kafka/data
      # Healthcheck
      - ./kafka/scripts/healthcheck.sh:/opt/kafka/scripts/healthcheck.sh:ro
      - ./kafka/scripts/logger.sh:/opt/kafka/scripts/logger.sh:ro
      - ./kafka/lib/jmxterm-1.0.2-uber.jar:/opt/jmxterm/jmxterm.jar:ro
      - ./kafka/conf/client.properties:/opt/kafka/config/client.properties:ro
      - ./kafka/secrets/kclient.keystore.jks:/opt/kafka/config/certs/kclient.keystore.jks:ro
      # Secrets
      - ./kafka/secrets/kafka.truststore.jks:/opt/kafka/config/certs/kafka.truststore.jks:ro
      - ./kafka/secrets/kafka2.keystore.jks:/opt/kafka/config/certs/kafka.keystore.jks:ro
    environment:
      # --- JMX ---
      KAFKA_JMX_OPTS: >
        -Djava.rmi.server.hostname=kafka2
        -Dcom.sun.management.jmxremote.port=2020
        -Dcom.sun.management.jmxremote.rmi.port=2020
        -Dcom.sun.management.jmxremote.authenticate=true
        -Dcom.sun.management.jmxremote.password.file=/opt/kafka/config/secrets/jmxremote.password
        -Dcom.sun.management.jmxremote.access.file=/opt/kafka/config/secrets/jmxremote.access
        -Dcom.sun.management.jmxremote.ssl=true
        -Dcom.sun.management.jmxremote.ssl.need.client.auth=false
        -Dcom.sun.management.jmxremote.registry.ssl=true
        -Djavax.net.ssl.keyStore=/opt/kafka/config/certs/kafka.keystore.jks
        -Djavax.net.ssl.keyStorePassword=${KAFKA_SSL_KEYSTORE_PASSWORD}
      JMX_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}

      # --- KRaft ---
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:9094,2@kafka2:9094,3@kafka3:9094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 9fnudAZ8ROCSiRomvzgeWg

      # --- LISTENERS ---
      KAFKA_LISTENERS: INTERNAL://:9091,CLIENT://:9092,BROKER://:9093,CONTROLLER://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:9091,CLIENT://kafka2:9092,BROKER://kafka2:9093,CONTROLLER://kafka2:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SSL,CLIENT:SASL_SSL,BROKER:SSL,CONTROLLER:SSL

      # --- BROKER-TO-BROKER & CONTROLLER ---
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # --- SSL / mTLS (listener-specific) ---
      # Broker (used for inter-broker replication) -> enforce mTLS
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_BROKER_SSL_PROTOCOL: TLSv1.3

      # Controller (controller quorum) -> enforce mTLS as well
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_PROTOCOL: TLSv1.3

      # Client listener uses TLS for encryption (clients perform SASL auth over TLS)
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_CLIENT_AUTH: none
      KAFKA_LISTENER_NAME_CLIENT_SSL_PROTOCOL: TLSv1.3

      # Internal listener (for tools, e.g. MirrorMaker2, Kafka Connect) - enforce mTLS
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_INTERNAL_SSL_PROTOCOL: TLSv1.3

      # --- SASL for CLIENT listener ---
      KAFKA_LISTENER_NAME_CLIENT_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENT_PLAIN_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        user_chat_admin="${KAFKA_USER_CHAT_ADMIN_PASSWORD}" \
        user_chat_prod_cons="${KAFKA_USER_CHAT_PROD_CONS_PASSWORD}" \
        user_chat_prod="${KAFKA_USER_CHAT_PRODUCER_PASSWORD}" \
        user_chat_cons="${KAFKA_USER_CHAT_CONSUMER_PASSWORD}" \
        user_schema_registry="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      # --- AUTHORIZATION ---
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_SUPER_USERS: User:kclient@manage;User:kafka1@manage;User:kafka2@manage;User:kafka3@manage
      KAFKA_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=(.*?),\s*OU=(.*?),.*$/$1@$2/L, DEFAULT
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: false

      # --- TOPIC / PERFORMANCE ---
      KAFKA_ELIGIBLE_LEADER_REPLICAS_VERSION: 1
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_CLEANER_THREADS: 3
      KAFKA_LOG_CLEANUP_POLICY: compact
      KAFKA_DELETE_TOPIC_ENABLE: true
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false

      # --- PRODUCER / CONSUMER ---
      KAFKA_MESSAGE_MAX_BYTES: 10485760         # 10 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760   # 10 MB
      KAFKA_FETCH_MAX_BYTES: 104857600          # 100 MB
      KAFKA_MAX_PARTITION_FETCH_BYTES: 10485760 # 10 MB

      # --- DATA RETENTION ---
      KAFKA_LOG_RETENTION_BYTES: 104857600  # 100 MB
      KAFKA_LOG_RETENTION_HOURS: 24         # 1 day
      KAFKA_LOG_ROLL_HOURS: 12              # 0.5 day
      KAFKA_LOG_ROLL_JITTER_HOURS: 1        # 1 hour
      KAFKA_LOG_SEGMENT_BYTES: 15728640     # 15 MB

      # --- LOGGING ---
      KAFKA_LOG4J_LOGGERS: >
        kafka=INFO,
        kafka.request.logger=ERROR,
        kafka.controller=INFO,
        kafka.log.LogCleaner=INFO,
        kafka.authorizer.logger=DEBUG,
        state.change.logger=INFO
    healthcheck:
      test: [ "CMD", "bash", "/opt/kafka/scripts/healthcheck.sh" ]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 4GB
    cpus: 4
    ulimits:
      nofile:
        soft: 100000
        hard: 100000

  kafka3:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka3
    hostname: kafka3
    logging: *default-logging
    networks:
      - kafka_backend_network
    volumes:
      # Data
      - kafka-data-3:/var/lib/kafka/data
      # Healthcheck
      - ./kafka/scripts/healthcheck.sh:/opt/kafka/scripts/healthcheck.sh:ro
      - ./kafka/scripts/logger.sh:/opt/kafka/scripts/logger.sh:ro
      - ./kafka/lib/jmxterm-1.0.2-uber.jar:/opt/jmxterm/jmxterm.jar:ro
      - ./kafka/conf/client.properties:/opt/kafka/config/client.properties:ro
      - ./kafka/secrets/kclient.keystore.jks:/opt/kafka/config/certs/kclient.keystore.jks:ro
      # Secrets
      - ./kafka/secrets/kafka.truststore.jks:/opt/kafka/config/certs/kafka.truststore.jks:ro
      - ./kafka/secrets/kafka3.keystore.jks:/opt/kafka/config/certs/kafka.keystore.jks:ro
    environment:
      # --- JMX ---
      KAFKA_JMX_OPTS: >
        -Djava.rmi.server.hostname=kafka3
        -Dcom.sun.management.jmxremote.port=2020
        -Dcom.sun.management.jmxremote.rmi.port=2020
        -Dcom.sun.management.jmxremote.authenticate=true
        -Dcom.sun.management.jmxremote.password.file=/opt/kafka/config/secrets/jmxremote.password
        -Dcom.sun.management.jmxremote.access.file=/opt/kafka/config/secrets/jmxremote.access
        -Dcom.sun.management.jmxremote.ssl=true
        -Dcom.sun.management.jmxremote.ssl.need.client.auth=false
        -Dcom.sun.management.jmxremote.registry.ssl=true
        -Djavax.net.ssl.keyStore=/opt/kafka/config/certs/kafka.keystore.jks
        -Djavax.net.ssl.keyStorePassword=${KAFKA_SSL_KEYSTORE_PASSWORD}
      JMX_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}

      # --- KRaft ---
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:9094,2@kafka2:9094,3@kafka3:9094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 9fnudAZ8ROCSiRomvzgeWg

      # --- LISTENERS ---
      KAFKA_LISTENERS: INTERNAL://:9091,CLIENT://:9092,BROKER://:9093,CONTROLLER://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:9091,CLIENT://kafka3:9092,BROKER://kafka3:9093,CONTROLLER://kafka3:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SSL,CLIENT:SASL_SSL,BROKER:SSL,CONTROLLER:SSL

      # --- BROKER-TO-BROKER & CONTROLLER ---
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # --- SSL / mTLS (listener-specific) ---
      # Broker (used for inter-broker replication) -> enforce mTLS
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_BROKER_SSL_PROTOCOL: TLSv1.3

      # Controller (controller quorum) -> enforce mTLS as well
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_PROTOCOL: TLSv1.3

      # Client listener uses TLS for encryption (clients perform SASL auth over TLS)
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_CLIENT_AUTH: none
      KAFKA_LISTENER_NAME_CLIENT_SSL_PROTOCOL: TLSv1.3

      # Internal listener (for tools, e.g. MirrorMaker2, Kafka Connect) - enforce mTLS
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_INTERNAL_SSL_PROTOCOL: TLSv1.3

      # --- SASL for CLIENT listener ---
      KAFKA_LISTENER_NAME_CLIENT_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENT_PLAIN_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        user_chat_admin="${KAFKA_USER_CHAT_ADMIN_PASSWORD}" \
        user_chat_prod_cons="${KAFKA_USER_CHAT_PROD_CONS_PASSWORD}" \
        user_chat_prod="${KAFKA_USER_CHAT_PRODUCER_PASSWORD}" \
        user_chat_cons="${KAFKA_USER_CHAT_CONSUMER_PASSWORD}" \
        user_schema_registry="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      # --- AUTHORIZATION ---
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_SUPER_USERS: User:kclient@manage;User:kafka1@manage;User:kafka2@manage;User:kafka3@manage
      KAFKA_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=(.*?),\s*OU=(.*?),.*$/$1@$2/L, DEFAULT
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: false

      # --- TOPIC / PERFORMANCE ---
      KAFKA_ELIGIBLE_LEADER_REPLICAS_VERSION: 1
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_CLEANER_THREADS: 3
      KAFKA_LOG_CLEANUP_POLICY: compact
      KAFKA_DELETE_TOPIC_ENABLE: true
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false

      # --- PRODUCER / CONSUMER ---
      KAFKA_MESSAGE_MAX_BYTES: 10485760         # 10 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760   # 10 MB
      KAFKA_FETCH_MAX_BYTES: 104857600          # 100 MB
      KAFKA_MAX_PARTITION_FETCH_BYTES: 10485760 # 10 MB

      # --- DATA RETENTION ---
      KAFKA_LOG_RETENTION_BYTES: 104857600  # 100 MB
      KAFKA_LOG_RETENTION_HOURS: 24         # 1 day
      KAFKA_LOG_ROLL_HOURS: 12              # 0.5 day
      KAFKA_LOG_ROLL_JITTER_HOURS: 1        # 1 hour
      KAFKA_LOG_SEGMENT_BYTES: 15728640     # 15 MB

      # --- LOGGING ---
      KAFKA_LOG4J_LOGGERS: >
        kafka=INFO,
        kafka.request.logger=ERROR,
        kafka.controller=INFO,
        kafka.log.LogCleaner=INFO,
        kafka.authorizer.logger=DEBUG,
        state.change.logger=INFO
    healthcheck:
      test: [ "CMD", "bash", "/opt/kafka/scripts/healthcheck.sh" ]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 4GB
    cpus: 4
    ulimits:
      nofile:
        soft: 100000
        hard: 100000

  schema-registry-1:
    image: 'confluentinc/cp-schema-registry:7.9.4'
    container_name: schema-registry-1
    hostname: schema-registry-1
    logging: *default-logging
    networks:
      - kafka_backend_network
    volumes:
      - ./kafka/secrets/kafka.truststore.jks:/etc/kafka/secrets/kafka.truststore.jks:ro
    environment:
      CUB_CLASSPATH: '/usr/share/java/confluent-security/schema-registry/*:/usr/share/java/schema-registry/*:/usr/share/java/schema-registry-plugins/*:/usr/share/java/cp-base-new/*'

      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1

      SCHEMA_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8085
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092

      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/kafka.truststore.jks
      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_ENABLED_PROTOCOLS: TLSv1.3
      SCHEMA_REGISTRY_KAFKASTORE_SSL_PROTOCOL: TLSv1.3

      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: SASL_SSL
      SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM: PLAIN
      SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="schema_registry" \
        password="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      SCHEMA_REGISTRY_EXPORTER_CONFIG_TOPIC: _csr_exporter_configs
      SCHEMA_REGISTRY_EXPORTER_STATE_TOPIC: _csr_exporter_states
      SCHEMA_REGISTRY_METADATA_ENCODER_TOPIC: _csr_schema_encoders
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _csr_schemas
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3

      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: full_transitive
    healthcheck:
      test: curl --fail --silent --insecure http://schema-registry-1:8085/subjects --output /dev/null || exit 1
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 10s
    restart: "no"
    mem_limit: 1GB
    cpus: 2
    depends_on: *kafka-dependencies

  nats-node-1:
    build:
      context: ./nats
    container_name: nats-node-1
    hostname: nats-node-1
    logging: *default-logging
    networks:
      - nats_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/nats/certs/ca/public.crt:ro
      - ./cert-generator/output/services/nats-node-1/private.key:/etc/nats/certs/private.key:ro
      - ./cert-generator/output/services/nats-node-1/public.crt:/etc/nats/certs/public.crt:ro
      # Data
      - nats-data-1:/var/lib/nats/jetstream
    command: >
      --https_port 8222
      --server_name nats-node-1
      --cluster_name chat-cluster
      --cluster_listen nats://0.0.0.0:6222
      --cluster_advertise nats-node-1:6222
      --routes nats://localhost:6222
      --tlsverify
      --tlscert /etc/nats/certs/public.crt
      --tlskey /etc/nats/certs/private.key
      --tlscacert /etc/nats/certs/ca/public.crt
      --user ${NATS_CHAT_APP_USER_USERNAME}
      --pass ${NATS_CHAT_APP_USER_PASSWORD}
      --jetstream
      --store_dir /var/lib/nats/jetstream
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "4222" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: "no"
    mem_limit: 2GB
    cpus: 4

  nats-node-2:
    build:
      context: ./nats
    container_name: nats-node-2
    hostname: nats-node-2
    logging: *default-logging
    networks:
      - nats_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/nats/certs/ca/public.crt:ro
      - ./cert-generator/output/services/nats-node-2/private.key:/etc/nats/certs/private.key:ro
      - ./cert-generator/output/services/nats-node-2/public.crt:/etc/nats/certs/public.crt:ro
      # Data
      - nats-data-2:/var/lib/nats/jetstream
    command: >
      --https_port 8222
      --server_name nats-node-2
      --cluster_name chat-cluster
      --cluster_listen nats://0.0.0.0:6222
      --cluster_advertise nats-node-2:6222
      --routes nats://nats-node-1:6222
      --tlsverify
      --tlscert /etc/nats/certs/public.crt
      --tlskey /etc/nats/certs/private.key
      --tlscacert /etc/nats/certs/ca/public.crt
      --user ${NATS_CHAT_APP_USER_USERNAME}
      --pass ${NATS_CHAT_APP_USER_PASSWORD}
      --jetstream
      --store_dir /var/lib/nats/jetstream
    healthcheck:
      test: [ "CMD-SHELL", "nc -z localhost 4222 && test $(curl -s --cacert /etc/nats/certs/ca/public.crt --cert /etc/nats/certs/public.crt --key /etc/nats/certs/private.key https://nats-node-2:8222/routez | jq 
      -r '.num_routes') -ge 2" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: "no"
    mem_limit: 2GB
    cpus: 4
    depends_on:
      nats-node-1:
        condition: service_healthy

  nats-node-3:
    build:
      context: ./nats
    container_name: nats-node-3
    hostname: nats-node-3
    logging: *default-logging
    networks:
      - nats_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/nats/certs/ca/public.crt:ro
      - ./cert-generator/output/services/nats-node-3/private.key:/etc/nats/certs/private.key:ro
      - ./cert-generator/output/services/nats-node-3/public.crt:/etc/nats/certs/public.crt:ro
      # Data
      - nats-data-3:/var/lib/nats/jetstream
    command: >
      --https_port 8222
      --server_name nats-node-3
      --cluster_name chat-cluster
      --cluster_listen nats://0.0.0.0:6222
      --cluster_advertise nats-node-3:6222
      --routes nats://nats-node-1:6222
      --tlsverify
      --tlscert /etc/nats/certs/public.crt
      --tlskey /etc/nats/certs/private.key
      --tlscacert /etc/nats/certs/ca/public.crt
      --user ${NATS_CHAT_APP_USER_USERNAME}
      --pass ${NATS_CHAT_APP_USER_PASSWORD}
      --jetstream
      --store_dir /var/lib/nats/jetstream
    healthcheck:
      test: [ "CMD-SHELL", "nc -z localhost 4222 && test $(curl -s --cacert /etc/nats/certs/ca/public.crt --cert /etc/nats/certs/public.crt --key /etc/nats/certs/private.key 
      https://nats-node-3:8222/routez | jq 
            -r '.num_routes') -ge 2" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: "no"
    mem_limit: 2GB
    cpus: 4
    depends_on:
      nats-node-1:
        condition: service_healthy

  mailpit:
    image: axllent/mailpit:v1.28.0
    container_name: mailpit
    hostname: mailpit
    logging: *default-logging
    networks:
      - mailpit_backend_network
    volumes:
      # Certificates
      - ./cert-generator/output/ca/root.crt:/etc/mailpit/certs/ca/public.crt:ro
      - ./cert-generator/output/services/mailpit/private.key:/etc/mailpit/certs/private.key:ro
      - ./cert-generator/output/services/mailpit/public.crt:/etc/mailpit/certs/public.crt:ro
      # Data
      - mailpit-data:/var/lib/mailpit
      # Secrets
      - ./mailpit/secrets:/etc/mailpit/secrets:ro
    ports:
      - 8025:8025
    environment:
      MP_DATABASE: /var/lib/mailpit/storage.db
      MP_DISABLE_VERSION_CHECK: true
      MP_COMPRESSION: 3
      MP_LABEL: local-dev
      MP_TENANT_ID: dev
      MP_MAX_MESSAGES: 1000
      MP_MAX_AGE: 1d

      MP_UI_BIND_ADDR: 0.0.0.0:8025
      MP_UI_AUTH_FILE: /etc/mailpit/secrets/ui.password
      MP_UI_TLS_CERT: /etc/mailpit/certs/public.crt
      MP_UI_TLS_KEY: /etc/mailpit/certs/private.key
      MP_SEND_API_AUTH_FILE: /etc/mailpit/secrets/http-send.password

      MP_SMTP_BIND_ADDR: 0.0.0.0:1025
      MP_SMTP_AUTH_FILE: /etc/mailpit/secrets/smtp.password
      MP_SMTP_TLS_CERT: /etc/mailpit/certs/public.crt
      MP_SMTP_TLS_KEY: /etc/mailpit/certs/private.key
      MP_SMTP_REQUIRE_TLS: true
      MP_SMTP_STRICT_RFC_HEADERS: true
      MP_SMTP_MAX_RECIPIENTS: 50
      MP_SMTP_DISABLE_RDNS: false

      MP_ENABLE_CHAOS: true
      MP_CHAOS_TRIGGERS: Sender:451:25,Recipients:550:25

      MP_POP3_BIND_ADDR: 0.0.0.0:1110
      MP_POP3_AUTH_FILE: /etc/mailpit/secrets/pop3.password
      MP_POP3_TLS_CERT: /etc/mailpit/certs/public.crt
      MP_POP3_TLS_KEY: /etc/mailpit/certs/private.key

      MP_TAG: '"message"="new message"'

      MP_ENABLE_PROMETHEUS: true
    healthcheck:
      test: [ "CMD", "nc", "-zv", "localhost", "1025" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: "no"
    mem_limit: 2GB
    cpus: 2

  prometheus:
    image: 'prom/prometheus:v3.5.0'
    container_name: prometheus
    hostname: prometheus
    logging: *default-logging
    networks:
      - monitoring_network
      - chat_app_network
    volumes:
      - prometheus-data:/var/lib/prometheus/data
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/var/lib/prometheus/data'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 2GB
    cpus: 4
    user: "0:0" # Run as root to ensure access to mounted config files

  grafana:
    image: 'grafana/grafana:12.3.0-17750354453'
    container_name: grafana
    hostname: grafana
    logging: *default-logging
    networks:
      - monitoring_network
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./grafana/provisioning/:/etc/grafana/provisioning/:ro
      - ./grafana/dashboards/:/etc/grafana/dashboards/:ro
      - grafana-data:/var/lib/grafana
    healthcheck:
      test: ["CMD", "curl", "-s", "--fail", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1GB
    cpus: 2
    depends_on:
      prometheus:
        condition: service_healthy

  chat-app-1:
    build:
      context: ./chat-app
      dockerfile: Dockerfile
    container_name: chat-app-1
    hostname: chat-app-1
    logging: *default-logging
    networks:
      - etcd_backend_network
      - postgres_backend_network
      - scylla_backend_network
      - redis_backend_network
      - elasticsearch_backend_network
      - neo4j_backend_network
      - kafka_backend_network
      - nats_backend_network
      - mailpit_backend_network
      - chat_app_network
    volumes:
      # Source code
      - ../../app:/usr/local/dev/chat
      # Scripts
      - ./chat-app/entrypoint.sh:/usr/local/bin/entrypoint.sh:ro
      # Config
      - ./chat-app/config.yaml:/etc/chat/config.yaml:ro
      - ./chat-app/templates/:/etc/chat/templates/:ro
      # Certificates
      ## Own
      - ./cert-generator/output/ca/root.crt:/etc/chat/certs/app/trusted/public.crt:ro
      - ./cert-generator/output/services/chat-app-1/public.crt:/etc/chat/certs/app/public.crt:ro
      - ./cert-generator/output/services/chat-app-1/private.key:/etc/chat/certs/app/private.key:ro
      ## Dependencies
      - ./kafka/secrets/ca.crt:/etc/chat/certs/kafka/trusted/public.crt:ro
      - type: volume
        source: es-setup-data-shared
        target: /etc/chat/certs/elasticsearch/trusted
        read_only: true
        volume:
          subpath: ca
    environment:
      ENV: development
      BUILD_VERSION: ${CHAT_APP_VERSION}
      BUILD_COMMIT: ${CHAT_APP_COMMIT}
      BUILD_TIME: ${CHAT_APP_BUILD_TIME}
    entrypoint: ["/bin/bash", "/usr/local/bin/entrypoint.sh"]
    mem_limit: 4GB # otherwise go build fails
    cpus: 4
    labels:
      "co.elastic.logs/enabled": true
      "co.elastic.logs/json.expand_keys": true
      "co.elastic.logs/json.add_error_key": true
      "co.elastic.logs/json.message_key": message
    depends_on:
      pgpool-node-1:
        condition: service_healthy
      scylla-setup:
        condition: service_healthy
      <<:
        - *redis-dependencies
        - *nats-dependencies
      es-filebeat:
        condition: service_healthy
      neo4j-node-1:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
      mailpit:
        condition: service_healthy
      grafana:
        condition: service_healthy
name: chatapp-cluster

networks:
  postgres_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.18.0.0/16
  scylla_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.19.0.0/16
  monitoring_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
  frontend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
  redis_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/16
  elasticsearch_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.23.0.0/16
  neo4j_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.24.0.0/16
  kafka_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16

volumes:
  etcd-data:
  pg-data-1:
  pg-data-2:
  pg-data-3:
  scylla-data-1:
  scylla-data-2:
  scylla-data-3:
  redis-data-1:
  redis-data-2:
  redis-data-3:
  redis-data-4:
  redis-data-5:
  redis-data-6:
  es-master-data-1:
  es-hot-data-1:
  es-warm-data-1:
  es-certs: # Shared Elasticsearch SSL certificates
  es-kibana-data:
  filebeat-data:
  neo4j-data-1:
  kafka-data-1:
  kafka-data-2:
  kafka-data-3:
  kafka_secrets:
  prometheus-data:
  grafana-data:

services:
  etcd:
    image: gcr.io/etcd-development/etcd:v3.6.4
    container_name: etcd
    hostname: etcd
    networks:
      - postgres_backend_network
    volumes:
      - etcd-data:/var/log/etcd
    command: >
      /usr/local/bin/etcd
      --name etcd
      --data-dir /var/log/etcd
      --listen-client-urls http://0.0.0.0:2379
      --advertise-client-urls http://etcd:2379
      --listen-peer-urls http://0.0.0.0:2380
      --initial-advertise-peer-urls http://etcd:2380
      --initial-cluster etcd=http://etcd:2380
      --log-level info
      --logger zap
      --log-outputs stderr
    healthcheck:
      test: ["CMD", "/usr/local/bin/etcdctl", "--endpoints=http://etcd:2379", "endpoint", "health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 512M
    cpus: 0.5

  # A pool of identical PostgreSQL nodes managed by Patroni.
  pg-node-1:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-1
    hostname: pg-node-1
    networks:
      - postgres_backend_network
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-1:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
    environment:
      - PATRONI_NAME=pg-node-1
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-1:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-1:5432
      - PGPOOL_BACKEND_NODE_ID=0
    depends_on:
      etcd:
        condition: service_healthy
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q"]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12

  pg-node-2:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-2
    hostname: pg-node-2
    networks:
      - postgres_backend_network
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-2:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
    environment:
      - PATRONI_NAME=pg-node-2
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-2:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-2:5432
      - PGPOOL_BACKEND_NODE_ID=1
    depends_on:
      etcd:
        condition: service_healthy
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: [ "CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q" ]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12

  pg-node-3:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-3
    hostname: pg-node-3
    networks:
      - postgres_backend_network
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-3:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
    environment:
      - PATRONI_NAME=pg-node-3
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-3:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-3:5432
      - PGPOOL_BACKEND_NODE_ID=2
    depends_on:
      etcd:
        condition: service_healthy
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: [ "CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q" ]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12

  pgpool:
    build:
      context: ./pgpool
      dockerfile: Dockerfile
    container_name: pgpool
    hostname: pgpool
    networks:
      - postgres_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./pgpool/conf/pcp.conf:/etc/pgpool2/pcp.conf
      - ./pgpool/conf/pgpool.conf:/tmp/pgpool.conf.template:ro
      - ./pgpool/conf/pool_hba.conf:/etc/pgpool2/pool_hba.conf:ro
      # SSL Certificates
      - ./cert-generator/output/services/pgpool/private.key:/etc/pgpool2/certs/private.key:ro
      - ./cert-generator/output/services/pgpool/public.crt:/etc/pgpool2/certs/public.crt:ro
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./pgpool/secrets/pgpoolkey.txt:/var/lib/postgresql/.pgpoolkey
      - ./pgpool/secrets/pcppass.txt:/var/lib/postgresql/.pcppass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass:ro
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_external_user_chat_ro.pass:ro
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_external_user_chat_rw.pass:ro
      # Scripts
      - ./pgpool/scripts/pre_init.sh:/etc/pgpool2/custom_pre_init.sh:ro
      - ./pgpool/scripts/follow_primary.sh:/etc/pgpool2/custom_follow_primary.sh:ro
      - ./pgpool/scripts/failover.sh:/etc/pgpool2/custom_failover.sh:ro
      - ./pgpool/scripts/failback.sh:/etc/pgpool2/custom_failback.sh:ro
    depends_on:
      pg-node-1:
        condition: service_healthy
      pg-node-2:
        condition: service_healthy
      pg-node-3:
        condition: service_healthy
    command: >
      sh -c "
        chown postgres:postgres /var/lib/postgresql/.pgpoolkey && chmod 400 /var/lib/postgresql/.pgpoolkey &&
        chown postgres:postgres /etc/pgpool2/pcp.conf && chmod 400 /etc/pgpool2/pcp.conf &&
        bash /etc/pgpool2/custom_pre_init.sh &&
        chown postgres:postgres /var/lib/postgresql/.pcppass && chmod 400 /var/lib/postgresql/.pcppass && 
        rm -f /tmp/pgpool.pid /tmp/.s.PGSQL.* && 
        exec gosu postgres /usr/sbin/pgpool -n -f /etc/pgpool2/pgpool.conf
      "
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_external_user_chat_ro.pass gosu postgres pg_isready -h pgpool -p 9999 -U chat_ro -d chat_db -q"]
      interval: 60s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1G
    cpus: 4

  pg-exporter:
    build:
      context: ./pg-exporter
      dockerfile: Dockerfile
    container_name: pg-exporter
    hostname: pg-exporter
    networks:
      - postgres_backend_network
      - monitoring_network
    volumes:
      - ./pg-exporter/postgres_exporter.yml:/tmp/postgres_exporter.yml.template:ro
      - ./pg-exporter/entrypoint.sh:/tmp/entrypoint.sh:ro
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass:ro
    entrypoint: [ '/tmp/entrypoint.sh' ]
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:9187/" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    depends_on:
      pgpool:
        condition: service_healthy
    mem_limit: 200MB
    cpus: 1

  scylla-node1:
    image: scylladb/scylla:2025.3
    container_name: scylla-node1
    hostname: scylla-node1
    networks:
      scylla_backend_network:
        aliases:
          - scylla-node1-internal # Give this node a specific name on the backend network
      frontend_network:
    volumes:
      - ./scylla/config/rack1.properties:/etc/scylla/cassandra-rackdc.properties:ro
      - scylla-data-1:/var/lib/scylla
    security_opt:
      - "seccomp:unconfined"
    command: >
      --seeds=scylla-node1-internal
      --listen-address=scylla-node1-internal
      --rpc-address=scylla-node1
      --native-shard-aware-transport-port=19042
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
    healthcheck:
      test: [ "CMD-SHELL", "nodetool status && sleep 10" ]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN

  scylla-node2:
    image: scylladb/scylla:2025.3
    container_name: scylla-node2
    hostname: scylla-node2
    networks:
      scylla_backend_network:
        aliases:
          - scylla-node2-internal
      frontend_network:
    volumes:
      - ./scylla/config/rack2.properties:/etc/scylla/cassandra-rackdc.properties:ro
      - scylla-data-2:/var/lib/scylla
    security_opt:
      - "seccomp:unconfined"
    command: >
      --seeds=scylla-node1-internal
      --listen-address=scylla-node2-internal
      --rpc-address=scylla-node2
      --native-shard-aware-transport-port=19042
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
    healthcheck:
      test: [ "CMD-SHELL", "nodetool status && sleep 5" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    depends_on:
      scylla-node1:
        condition: service_healthy
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN

  scylla-node3:
    image: scylladb/scylla:2025.3
    container_name: scylla-node3
    hostname: scylla-node3
    networks:
      scylla_backend_network:
        aliases:
          - scylla-node3-internal
      frontend_network:
    volumes:
      - ./scylla/config/rack3.properties:/etc/scylla/cassandra-rackdc.properties:ro
      - scylla-data-3:/var/lib/scylla
    security_opt:
      - "seccomp:unconfined"
    command: >
      --seeds=scylla-node1-internal
      --listen-address=scylla-node3-internal
      --rpc-address=scylla-node3
      --native-shard-aware-transport-port=19042
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
    healthcheck:
      test: [ "CMD-SHELL", "nodetool status && sleep 5" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    depends_on:
      scylla-node2:
        condition: service_healthy
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN

  scylla-maintainer:
    image: scylladb/scylla:2025.3
    container_name: scylla-maintainer
    networks:
      - scylla_backend_network
    volumes:
      - ./scylla/scripts/init.cql:/tmp/init.cql:ro
      - ./scylla/scripts/init.sh:/tmp/init.sh:ro
    env_file:
      - ./scylla/secrets/.env.secrets
    entrypoint: sh /tmp/init.sh
    restart: no
    depends_on:
      scylla-node1:
        condition: service_healthy
      scylla-node2:
        condition: service_healthy
      scylla-node3:
        condition: service_healthy

  redis-node-1:
    image: redis:8.2.1
    container_name: redis-node-1
    hostname: redis-node-1
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-1/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-1/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-1:/data
    entrypoint: /usr/local/etc/redis/scripts/entrypoint.sh
    command: >
      /usr/local/etc/redis/redis.conf 
      --cluster-announce-hostname redis-node-1
      --cluster-announce-human-nodename redis-node-1
      --cluster-announce-ip redis-node-1
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-2:
    image: redis:8.2.1
    container_name: redis-node-2
    hostname: redis-node-2
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-2/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-2/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-2:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-2
      --cluster-announce-human-nodename redis-node-2
      --cluster-announce-ip redis-node-2
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-3:
    image: redis:8.2.1
    container_name: redis-node-3
    hostname: redis-node-3
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-3/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-3/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-3:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-3
      --cluster-announce-human-nodename redis-node-3
      --cluster-announce-ip redis-node-3
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-4:
    image: redis:8.2.1
    container_name: redis-node-4
    hostname: redis-node-4
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-4/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-4/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-4:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-4
      --cluster-announce-human-nodename redis-node-4
      --cluster-announce-ip redis-node-4
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-5:
    image: redis:8.2.1
    container_name: redis-node-5
    hostname: redis-node-5
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-5/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-5/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-5:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-5
      --cluster-announce-human-nodename redis-node-5
      --cluster-announce-ip redis-node-5
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-6:
    image: redis:8.2.1
    container_name: redis-node-6
    hostname: redis-node-6
    networks:
      - redis_backend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Certificates
      - ./cert-generator/output/ca/root.crt:/usr/local/etc/redis/certs/ca/public.crt:ro
      - ./cert-generator/output/services/redis-node-6/private.key:/usr/local/etc/redis/certs/private.key:ro
      - ./cert-generator/output/services/redis-node-6/public.crt:/usr/local/etc/redis/certs/public.crt:ro
      # Scripts
      - ./redis/scripts:/usr/local/etc/redis/scripts/:ro
      # Data
      - redis-data-6:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --cluster-announce-hostname redis-node-6
      --cluster-announce-human-nodename redis-node-6
      --cluster-announce-ip redis-node-6
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  es-setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-setup
    hostname: es-setup
    networks:
      - elasticsearch_backend_network
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - ./elasticsearch/certs/instances.yml:/usr/share/elasticsearch/config/certs/instances.yml
      - ./elasticsearch/scripts/init.sh:/tmp/scripts/init.sh:ro
      - ./redis/scripts/logger.sh:/tmp/scripts/logger.sh:ro
      - ./elasticsearch/schemas/chat-messages/ilm.json:/tmp/schemas/chat-messages/ilm.json:ro
      - ./elasticsearch/schemas/chat-messages/settings.json:/tmp/schemas/chat-messages/settings.json:ro
      - ./elasticsearch/schemas/chat-messages/mappings.json:/tmp/schemas/chat-messages/mappings.json:ro
      - ./elasticsearch/schemas/chat-messages/template.json:/tmp/schemas/chat-messages/template.json:ro
    environment:
      - ELASTICSEARCH_ELASTIC_USERNAME=${ELASTICSEARCH_ELASTIC_USERNAME}
      - ELASTICSEARCH_ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - ELASTICSEARCH_KIBANA_SYSTEM_USERNAME=${ELASTICSEARCH_KIBANA_SYSTEM_USERNAME}
      - ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD=${ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD}
      - ELASTICSEARCH_KIBANA_ADMIN_USERNAME=${ELASTICSEARCH_KIBANA_ADMIN_USERNAME}
      - ELASTICSEARCH_KIBANA_ADMIN_PASSWORD=${ELASTICSEARCH_KIBANA_ADMIN_PASSWORD}
      - ELASTICSEARCH_CHAT_APP_USERNAME=${ELASTICSEARCH_CHAT_APP_USERNAME}
      - ELASTICSEARCH_CHAT_APP_PASSWORD=${ELASTICSEARCH_CHAT_APP_PASSWORD}
      - ELASTICSEARCH_FILEBEAT_WRITER_USERNAME=${ELASTICSEARCH_FILEBEAT_WRITER_USERNAME}
      - ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD=${ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD}
      - ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME=${ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME}
      - ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD=${ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD}
    user: "0"
    entrypoint: [ "/bin/bash", "/tmp/scripts/init.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "[ -f config/certs/es-master-1/es-master-1.crt ]" ]
      interval: 10s
      timeout: 5s
      retries: 120
    restart: no
    mem_limit: 200M
    cpus: 1

  es-master-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-master-1
    hostname: es-master-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.11
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - es-master-data-1:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.11
      - node.name=es-master-1
      - node.roles=master
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=""
      - cluster.initial_master_nodes=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true # Rule: Disable swapping for performance
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" # Rule: Set -Xms and -Xmx equal. Use small heap for master nodes.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-master-1/es-master-1.key
      - xpack.security.http.ssl.certificate=certs/es-master-1/es-master-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-master-1/es-master-1.key
      - xpack.security.transport.ssl.certificate=certs/es-master-1/es-master-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL","curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-master-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 2GB
    cpus: 1
    depends_on:
      es-setup: { condition: service_healthy }

  es-data-hot-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-data-hot-1
    hostname: es-data-hot-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.21
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - es-hot-data-1:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.21
      - node.name=es-data-hot-1
      - node.roles=data
      - node.attr.data_tier=hot
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g" # IMPORTANT: Set this to <= 50% of the host RAM, but do not exceed ~31GB.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-data-hot-1/es-data-hot-1.key
      - xpack.security.http.ssl.certificate=certs/es-data-hot-1/es-data-hot-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-data-hot-1/es-data-hot-1.key
      - xpack.security.transport.ssl.certificate=certs/es-data-hot-1/es-data-hot-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-data-hot-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 6GB
    cpus: 6
    depends_on:
      es-setup: { condition: service_healthy }

  es-data-warm-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-data-warm-1
    hostname: es-data-warm-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.22
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - es-warm-data-1:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.22
      - node.name=es-data-warm-1
      - node.roles=data
      - node.attr.data_tier=warm
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g" # IMPORTANT: Set this to <= 50% of the host RAM, but do not exceed ~31GB.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-data-warm-1/es-data-warm-1.key
      - xpack.security.http.ssl.certificate=certs/es-data-warm-1/es-data-warm-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-data-warm-1/es-data-warm-1.key
      - xpack.security.transport.ssl.certificate=certs/es-data-warm-1/es-data-warm-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-data-warm-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 6GB
    cpus: 6
    depends_on:
      es-setup: { condition: service_healthy }

  es-coordinating-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-coordinating-1
    hostname: es-coordinating-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.31
      frontend_network:
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
    environment:
      - network.host=0.0.0.0
      - network.publish_host=172.23.0.31
      - node.name=es-coordinating-1
      - node.roles="" # Empty role makes it coordinating-only
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms800m -Xmx800m" # Moderate heap for query aggregation.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-coordinating-1/es-coordinating-1.key
      - xpack.security.http.ssl.certificate=certs/es-coordinating-1/es-coordinating-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-coordinating-1/es-coordinating-1.key
      - xpack.security.transport.ssl.certificate=certs/es-coordinating-1/es-coordinating-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-coordinating-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 4GB
    cpus: 4
    depends_on:
      es-setup: { condition: service_healthy }

  es-kibana:
    image: docker.elastic.co/kibana/kibana:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-kibana
    hostname: es-kibana
    ports:
      - "5781:5601"
    networks:
      - elasticsearch_backend_network
    volumes:
      - es-kibana-data:/usr/share/kibana/data
      - es-certs:/usr/share/kibana/config/certs
    environment:
      - SERVERNAME=es-kibana
      - ELASTICSEARCH_HOSTS=https://es-coordinating-1:9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_KIBANA_SYSTEM_USERNAME}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=/usr/share/kibana/config/certs/ca/ca.crt
    healthcheck:
      test: [ "CMD-SHELL", "curl -s --fail http://localhost:5601/api/status || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1GB
    cpus: 2
    depends_on:
      es-setup: { condition: service_healthy }

  filebeat:
    image: docker.elastic.co/beats/filebeat-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: filebeat
    hostname: filebeat
    user: root # Required to access Docker log files and socket
    networks:
      - elasticsearch_backend_network
    volumes:
      # --- Mount the scripts ---
      - ./filebeat/scripts/entrypoint.sh:/usr/local/bin/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/tmp/scripts/logger.sh:ro
      # Mounts for collecting logs and metadata from the Docker daemon
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      # Mount shared certificates for secure connection to Elasticsearch
      - es-certs:/usr/share/filebeat/config/certs:ro
      # Mount the configuration file
      - ./filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml
      # Persist Filebeat's registry data to track log file positions
      - filebeat-data:/usr/share/filebeat/data
    environment:
      # --- Credentials for the 'setup' command ---
      - KIBANA_HOST=es-kibana:5601
      - ELASTICSEARCH_ELASTIC_USERNAME=${ELASTICSEARCH_ELASTIC_USERNAME}
      - ELASTICSEARCH_ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      # Credentials for sending logs to Elasticsearch
      - ELASTICSEARCH_HOSTS=https://es-coordinating-1:9200
      - ELASTICSEARCH_FILEBEAT_WRITER_USERNAME=${ELASTICSEARCH_FILEBEAT_WRITER_USERNAME}
      - ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD=${ELASTICSEARCH_FILEBEAT_WRITER_PASSWORD}
      # Credentials for sending Filebeat's own monitoring data
      - ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME=${ELASTICSEARCH_FILEBEAT_MONITORING_USERNAME}
      - ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD=${ELASTICSEARCH_FILEBEAT_MONITORING_PASSWORD}
    entrypoint: ["/usr/local/bin/entrypoint.sh"]
    healthcheck:
      test: [ "CMD-SHELL", "filebeat test output" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 1GB
    cpus: 2
    depends_on:
      es-coordinating-1:
        condition: service_healthy
      es-kibana:
        condition: service_healthy

  neo4j-node-1:
    image: neo4j:5.26.12-enterprise-ubi9
    container_name: neo4j-node-1
    hostname: neo4j-node-1
    networks:
      neo4j_backend_network:
      frontend_network:
    ports:
      - "7473:7473"   # HTTPS
    volumes:
      - neo4j-data-1:/data
      - ./neo4j/certs:/var/lib/neo4j/certificates/bolt
      - ./neo4j/certs:/var/lib/neo4j/certificates/https
      - ./neo4j/scripts/setup.sh:/startup/setup.sh:ro
      - ./neo4j/scripts/entrypoint.sh:/startup/entrypoint.sh:ro
      - ./neo4j/schema/chat.cypher:/schema/chat.cypher:ro
      - ./redis/scripts/logger.sh:/tmp/scripts/logger.sh:ro
    environment:
      # --- Authentication & License ---
      # This password is used by the setup script and should be changed or secured.
      - NEO4J_AUTH=neo4j/${NEO4J_NEO4J_USER_PASSWORD}
      - SCRIPT_PASSWORD_NEO4J=${NEO4J_NEO4J_USER_PASSWORD}
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes

      # --- Performance Tuning: Memory ---
      - NEO4J_server_memory_heap_initial__size=1G
      - NEO4J_server_memory_heap_max__size=1G
      - NEO4J_server_memory_pagecache_size=2.5G

      # --- Performance Tuning: Checkpointing & Transactions ---
      - NEO4J_db_checkpoint=VOLUMETRIC # Sets checkpointing policy based on transaction log volume, ideal for write-heavy loads.
      - NEO4J_db_lock_acquisition_timeout=10s # Sets a timeout for transactions waiting to acquire a lock, preventing deadlocks.
      - NEO4J_db_tx__log_rotation_retention__policy=2 days 2G # Specifies how long to keep transaction logs.

      # --- Security and Networking ---
      - NEO4J_server_default__listen__address=0.0.0.0
      - NEO4J_dbms_netty_ssl_provider=OPENSSL
      - NEO4J_server_bolt_enabled=true
      - NEO4J_server_bolt_tls__level=REQUIRED
      - NEO4J_dbms_ssl_policy_bolt_enabled=true
      - NEO4J_dbms_ssl_policy_bolt_base_directory=/var/lib/neo4j/certificates/bolt
      - NEO4J_dbms_ssl_policy_bolt_private_key=private.key
      - NEO4J_dbms_ssl_policy_bolt_public_certificate=public.crt
      - NEO4J_dbms_ssl_policy_bolt_client_auth=NONE
      - NEO4J_server_http_enabled=false
      - NEO4J_server_https_enabled=true
      - NEO4J_dbms_ssl_policy_https_enabled=true
      - NEO4J_dbms_ssl_policy_https_base_directory=/var/lib/neo4j/certificates/https
      - NEO4J_dbms_ssl_policy_https_private_key=private.key
      - NEO4J_dbms_ssl_policy_https_public_certificate=public.crt
      - NEO4J_dbms_ssl_policy_https_client_auth=NONE

      # --- Monitoring ---
      - NEO4J_server_metrics_prometheus_enabled=false

      # --- Connectivity ---
      - NEO4J_initial_dbms_default__database=chatdb
    entrypoint: [ "/startup/entrypoint.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "cypher-shell -a bolt+ssc://localhost:7687 -u neo4j -p ${NEO4J_NEO4J_USER_PASSWORD} 'RETURN 1'" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 4GB
    cpus: 4

  kafka1:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka1
    hostname: kafka1
    networks:
      kafka_backend_network:
      frontend_network:
    ulimits:
      nofile:
        soft: 100000
        hard: 100000
    volumes:
      # Data
      - kafka-data-1:/var/lib/kafka/data
      # Healthcheck
      - ./kafka/scripts/healthcheck.sh:/opt/kafka/scripts/healthcheck.sh:ro
      - ./kafka/scripts/logger.sh:/opt/kafka/scripts/logger.sh:ro
      - ./kafka/lib/jmxterm-1.0.2-uber.jar:/opt/jmxterm/jmxterm.jar:ro
      - ./kafka/conf/client.properties:/opt/kafka/config/client.properties:ro
      - ./kafka/secrets/kclient.keystore.jks:/opt/kafka/config/certs/kclient.keystore.jks:ro
      # Secrets
      - ./kafka/secrets/kafka.truststore.jks:/opt/kafka/config/certs/kafka.truststore.jks:ro
      - ./kafka/secrets/kafka1.keystore.jks:/opt/kafka/config/certs/kafka.keystore.jks:ro
    environment:
      # --- JMX ---
      KAFKA_JMX_OPTS: >
        -Djava.rmi.server.hostname=kafka1
        -Dcom.sun.management.jmxremote.port=2020
        -Dcom.sun.management.jmxremote.rmi.port=2020
        -Dcom.sun.management.jmxremote.authenticate=true
        -Dcom.sun.management.jmxremote.password.file=/opt/kafka/config/secrets/jmxremote.password
        -Dcom.sun.management.jmxremote.access.file=/opt/kafka/config/secrets/jmxremote.access
        -Dcom.sun.management.jmxremote.ssl=true
        -Dcom.sun.management.jmxremote.ssl.need.client.auth=false
        -Dcom.sun.management.jmxremote.registry.ssl=true
        -Djavax.net.ssl.keyStore=/opt/kafka/config/certs/kafka.keystore.jks
        -Djavax.net.ssl.keyStorePassword=${KAFKA_SSL_KEYSTORE_PASSWORD}
      JMX_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}

      # --- KRaft ---
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:9094,2@kafka2:9094,3@kafka3:9094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 9fnudAZ8ROCSiRomvzgeWg

      # --- LISTENERS ---
      KAFKA_LISTENERS: INTERNAL://:9091,CLIENT://:9092,BROKER://:9093,CONTROLLER://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:9091,CLIENT://kafka1:9092,BROKER://kafka1:9093,CONTROLLER://kafka1:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SSL,CLIENT:SASL_SSL,BROKER:SSL,CONTROLLER:SSL

      # --- BROKER-TO-BROKER & CONTROLLER ---
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # --- SSL / mTLS (listener-specific) ---
      # Broker (used for inter-broker replication) -> enforce mTLS
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_BROKER_SSL_PROTOCOL: TLSv1.3

      # Controller (controller quorum) -> enforce mTLS as well
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_PROTOCOL: TLSv1.3

      # Client listener uses TLS for encryption (clients perform SASL auth over TLS)
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_CLIENT_AUTH: none
      KAFKA_LISTENER_NAME_CLIENT_SSL_PROTOCOL: TLSv1.3

      # Internal listener (for tools, e.g. MirrorMaker2, Kafka Connect) - enforce mTLS
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_INTERNAL_SSL_PROTOCOL: TLSv1.3

      # --- SASL for CLIENT listener ---
      KAFKA_LISTENER_NAME_CLIENT_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENT_PLAIN_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        user_chat_admin="${KAFKA_USER_CHAT_ADMIN_PASSWORD}" \
        user_chat_prod_cons="${KAFKA_USER_CHAT_PROD_CONS_PASSWORD}" \
        user_chat_prod="${KAFKA_USER_CHAT_PRODUCER_PASSWORD}" \
        user_chat_cons="${KAFKA_USER_CHAT_CONSUMER_PASSWORD}" \
        user_schema_registry="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      # --- AUTHORIZATION ---
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_SUPER_USERS: User:kclient@manage;User:kafka1@manage;User:kafka2@manage;User:kafka3@manage
      KAFKA_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=(.*?),\s*OU=(.*?),.*$/$1@$2/L, DEFAULT
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: false

      # --- TOPIC / PERFORMANCE ---
      KAFKA_ELIGIBLE_LEADER_REPLICAS_VERSION: 1
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_CLEANER_THREADS: 3
      KAFKA_LOG_CLEANUP_POLICY: compact
      KAFKA_DELETE_TOPIC_ENABLE: true
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false

      # --- PRODUCER / CONSUMER ---
      KAFKA_MESSAGE_MAX_BYTES: 10485760         # 10 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760   # 10 MB
      KAFKA_FETCH_MAX_BYTES: 104857600          # 100 MB
      KAFKA_MAX_PARTITION_FETCH_BYTES: 10485760 # 10 MB

      # --- DATA RETENTION ---
      KAFKA_LOG_RETENTION_BYTES: 104857600  # 100 MB
      KAFKA_LOG_RETENTION_HOURS: 24         # 1 day
      KAFKA_LOG_ROLL_HOURS: 12              # 0.5 day
      KAFKA_LOG_ROLL_JITTER_HOURS: 1        # 1 hour
      KAFKA_LOG_SEGMENT_BYTES: 15728640     # 15 MB

      # --- LOGGING ---
      KAFKA_LOG4J_LOGGERS: >
        kafka=INFO,
        kafka.request.logger=ERROR,
        kafka.controller=INFO,
        kafka.log.LogCleaner=INFO,
        kafka.authorizer.logger=DEBUG,
        state.change.logger=INFO
    healthcheck:
      test: [ "CMD", "bash", "/opt/kafka/scripts/healthcheck.sh" ]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 4GB
    cpus: 4

  kafka2:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka2
    hostname: kafka2
    networks:
      kafka_backend_network:
      frontend_network:
    ulimits:
      nofile:
        soft: 100000
        hard: 100000
    volumes:
      # Data
      - kafka-data-2:/var/lib/kafka/data
      # Healthcheck
      - ./kafka/scripts/healthcheck.sh:/opt/kafka/scripts/healthcheck.sh:ro
      - ./kafka/scripts/logger.sh:/opt/kafka/scripts/logger.sh:ro
      - ./kafka/lib/jmxterm-1.0.2-uber.jar:/opt/jmxterm/jmxterm.jar:ro
      - ./kafka/conf/client.properties:/opt/kafka/config/client.properties:ro
      - ./kafka/secrets/kclient.keystore.jks:/opt/kafka/config/certs/kclient.keystore.jks:ro
      # Secrets
      - ./kafka/secrets/kafka.truststore.jks:/opt/kafka/config/certs/kafka.truststore.jks:ro
      - ./kafka/secrets/kafka2.keystore.jks:/opt/kafka/config/certs/kafka.keystore.jks:ro
    environment:
      # --- JMX ---
      KAFKA_JMX_OPTS: >
        -Djava.rmi.server.hostname=kafka2
        -Dcom.sun.management.jmxremote.port=2020
        -Dcom.sun.management.jmxremote.rmi.port=2020
        -Dcom.sun.management.jmxremote.authenticate=true
        -Dcom.sun.management.jmxremote.password.file=/opt/kafka/config/secrets/jmxremote.password
        -Dcom.sun.management.jmxremote.access.file=/opt/kafka/config/secrets/jmxremote.access
        -Dcom.sun.management.jmxremote.ssl=true
        -Dcom.sun.management.jmxremote.ssl.need.client.auth=false
        -Dcom.sun.management.jmxremote.registry.ssl=true
        -Djavax.net.ssl.keyStore=/opt/kafka/config/certs/kafka.keystore.jks
        -Djavax.net.ssl.keyStorePassword=${KAFKA_SSL_KEYSTORE_PASSWORD}
      JMX_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}

      # --- KRaft ---
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:9094,2@kafka2:9094,3@kafka3:9094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 9fnudAZ8ROCSiRomvzgeWg

      # --- LISTENERS ---
      KAFKA_LISTENERS: INTERNAL://:9091,CLIENT://:9092,BROKER://:9093,CONTROLLER://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:9091,CLIENT://kafka2:9092,BROKER://kafka2:9093,CONTROLLER://kafka2:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SSL,CLIENT:SASL_SSL,BROKER:SSL,CONTROLLER:SSL

      # --- BROKER-TO-BROKER & CONTROLLER ---
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # --- SSL / mTLS (listener-specific) ---
      # Broker (used for inter-broker replication) -> enforce mTLS
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_BROKER_SSL_PROTOCOL: TLSv1.3

      # Controller (controller quorum) -> enforce mTLS as well
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_PROTOCOL: TLSv1.3

      # Client listener uses TLS for encryption (clients perform SASL auth over TLS)
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_CLIENT_AUTH: none
      KAFKA_LISTENER_NAME_CLIENT_SSL_PROTOCOL: TLSv1.3

      # Internal listener (for tools, e.g. MirrorMaker2, Kafka Connect) - enforce mTLS
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_INTERNAL_SSL_PROTOCOL: TLSv1.3

      # --- SASL for CLIENT listener ---
      KAFKA_LISTENER_NAME_CLIENT_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENT_PLAIN_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        user_chat_admin="${KAFKA_USER_CHAT_ADMIN_PASSWORD}" \
        user_chat_prod_cons="${KAFKA_USER_CHAT_PROD_CONS_PASSWORD}" \
        user_chat_prod="${KAFKA_USER_CHAT_PRODUCER_PASSWORD}" \
        user_chat_cons="${KAFKA_USER_CHAT_CONSUMER_PASSWORD}" \
        user_schema_registry="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      # --- AUTHORIZATION ---
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_SUPER_USERS: User:kclient@manage;User:kafka1@manage;User:kafka2@manage;User:kafka3@manage
      KAFKA_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=(.*?),\s*OU=(.*?),.*$/$1@$2/L, DEFAULT
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: false

      # --- TOPIC / PERFORMANCE ---
      KAFKA_ELIGIBLE_LEADER_REPLICAS_VERSION: 1
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_CLEANER_THREADS: 3
      KAFKA_LOG_CLEANUP_POLICY: compact
      KAFKA_DELETE_TOPIC_ENABLE: true
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false

      # --- PRODUCER / CONSUMER ---
      KAFKA_MESSAGE_MAX_BYTES: 10485760         # 10 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760   # 10 MB
      KAFKA_FETCH_MAX_BYTES: 104857600          # 100 MB
      KAFKA_MAX_PARTITION_FETCH_BYTES: 10485760 # 10 MB

      # --- DATA RETENTION ---
      KAFKA_LOG_RETENTION_BYTES: 104857600  # 100 MB
      KAFKA_LOG_RETENTION_HOURS: 24         # 1 day
      KAFKA_LOG_ROLL_HOURS: 12              # 0.5 day
      KAFKA_LOG_ROLL_JITTER_HOURS: 1        # 1 hour
      KAFKA_LOG_SEGMENT_BYTES: 15728640     # 15 MB

      # --- LOGGING ---
      KAFKA_LOG4J_LOGGERS: >
        kafka=INFO,
        kafka.request.logger=ERROR,
        kafka.controller=INFO,
        kafka.log.LogCleaner=INFO,
        kafka.authorizer.logger=DEBUG,
        state.change.logger=INFO
    healthcheck:
      test: [ "CMD", "bash", "/opt/kafka/scripts/healthcheck.sh" ]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 4GB
    cpus: 4

  kafka3:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka3
    hostname: kafka3
    networks:
      kafka_backend_network:
      frontend_network:
    ulimits:
      nofile:
        soft: 100000
        hard: 100000
    volumes:
      # Data
      - kafka-data-3:/var/lib/kafka/data
      # Healthcheck
      - ./kafka/scripts/healthcheck.sh:/opt/kafka/scripts/healthcheck.sh:ro
      - ./kafka/scripts/logger.sh:/opt/kafka/scripts/logger.sh:ro
      - ./kafka/lib/jmxterm-1.0.2-uber.jar:/opt/jmxterm/jmxterm.jar:ro
      - ./kafka/conf/client.properties:/opt/kafka/config/client.properties:ro
      - ./kafka/secrets/kclient.keystore.jks:/opt/kafka/config/certs/kclient.keystore.jks:ro
      # Secrets
      - ./kafka/secrets/kafka.truststore.jks:/opt/kafka/config/certs/kafka.truststore.jks:ro
      - ./kafka/secrets/kafka3.keystore.jks:/opt/kafka/config/certs/kafka.keystore.jks:ro
    environment:
      # --- JMX ---
      KAFKA_JMX_OPTS: >
        -Djava.rmi.server.hostname=kafka3
        -Dcom.sun.management.jmxremote.port=2020
        -Dcom.sun.management.jmxremote.rmi.port=2020
        -Dcom.sun.management.jmxremote.authenticate=true
        -Dcom.sun.management.jmxremote.password.file=/opt/kafka/config/secrets/jmxremote.password
        -Dcom.sun.management.jmxremote.access.file=/opt/kafka/config/secrets/jmxremote.access
        -Dcom.sun.management.jmxremote.ssl=true
        -Dcom.sun.management.jmxremote.ssl.need.client.auth=false
        -Dcom.sun.management.jmxremote.registry.ssl=true
        -Djavax.net.ssl.keyStore=/opt/kafka/config/certs/kafka.keystore.jks
        -Djavax.net.ssl.keyStorePassword=${KAFKA_SSL_KEYSTORE_PASSWORD}
      JMX_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}

      # --- KRaft ---
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:9094,2@kafka2:9094,3@kafka3:9094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 9fnudAZ8ROCSiRomvzgeWg

      # --- LISTENERS ---
      KAFKA_LISTENERS: INTERNAL://:9091,CLIENT://:9092,BROKER://:9093,CONTROLLER://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:9091,CLIENT://kafka3:9092,BROKER://kafka3:9093,CONTROLLER://kafka3:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SSL,CLIENT:SASL_SSL,BROKER:SSL,CONTROLLER:SSL

      # --- BROKER-TO-BROKER & CONTROLLER ---
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # --- SSL / mTLS (listener-specific) ---
      # Broker (used for inter-broker replication) -> enforce mTLS
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_BROKER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_BROKER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_BROKER_SSL_PROTOCOL: TLSv1.3

      # Controller (controller quorum) -> enforce mTLS as well
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_PROTOCOL: TLSv1.3

      # Client listener uses TLS for encryption (clients perform SASL auth over TLS)
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_CLIENT_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_CLIENT_SSL_CLIENT_AUTH: none
      KAFKA_LISTENER_NAME_CLIENT_SSL_PROTOCOL: TLSv1.3

      # Internal listener (for tools, e.g. MirrorMaker2, Kafka Connect) - enforce mTLS
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_LOCATION: /opt/kafka/config/certs/kafka.keystore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_LOCATION: /opt/kafka/config/certs/kafka.truststore.jks
      KAFKA_LISTENER_NAME_INTERNAL_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      KAFKA_LISTENER_NAME_INTERNAL_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_INTERNAL_SSL_PROTOCOL: TLSv1.3

      # --- SASL for CLIENT listener ---
      KAFKA_LISTENER_NAME_CLIENT_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_CLIENT_PLAIN_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        user_chat_admin="${KAFKA_USER_CHAT_ADMIN_PASSWORD}" \
        user_chat_prod_cons="${KAFKA_USER_CHAT_PROD_CONS_PASSWORD}" \
        user_chat_prod="${KAFKA_USER_CHAT_PRODUCER_PASSWORD}" \
        user_chat_cons="${KAFKA_USER_CHAT_CONSUMER_PASSWORD}" \
        user_schema_registry="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      # --- AUTHORIZATION ---
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_SUPER_USERS: User:kclient@manage;User:kafka1@manage;User:kafka2@manage;User:kafka3@manage
      KAFKA_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=(.*?),\s*OU=(.*?),.*$/$1@$2/L, DEFAULT
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: false

      # --- TOPIC / PERFORMANCE ---
      KAFKA_ELIGIBLE_LEADER_REPLICAS_VERSION: 1
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_CLEANER_THREADS: 3
      KAFKA_LOG_CLEANUP_POLICY: compact
      KAFKA_DELETE_TOPIC_ENABLE: true
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false

      # --- PRODUCER / CONSUMER ---
      KAFKA_MESSAGE_MAX_BYTES: 10485760         # 10 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760   # 10 MB
      KAFKA_FETCH_MAX_BYTES: 104857600          # 100 MB
      KAFKA_MAX_PARTITION_FETCH_BYTES: 10485760 # 10 MB

      # --- DATA RETENTION ---
      KAFKA_LOG_RETENTION_BYTES: 104857600  # 100 MB
      KAFKA_LOG_RETENTION_HOURS: 24         # 1 day
      KAFKA_LOG_ROLL_HOURS: 12              # 0.5 day
      KAFKA_LOG_ROLL_JITTER_HOURS: 1        # 1 hour
      KAFKA_LOG_SEGMENT_BYTES: 15728640     # 15 MB

      # --- LOGGING ---
      KAFKA_LOG4J_LOGGERS: >
        kafka=INFO,
        kafka.request.logger=ERROR,
        kafka.controller=INFO,
        kafka.log.LogCleaner=INFO,
        kafka.authorizer.logger=DEBUG,
        state.change.logger=INFO
    healthcheck:
      test: [ "CMD", "bash", "/opt/kafka/scripts/healthcheck.sh" ]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 4GB
    cpus: 4

  schema-registry-1:
    image: 'confluentinc/cp-schema-registry:7.9.4'
    container_name: schema-registry-1
    hostname: schema-registry-1
    networks:
      - kafka_backend_network
      - frontend_network
    volumes:
      - ./kafka/secrets/kafka.truststore.jks:/etc/kafka/secrets/kafka.truststore.jks:ro
    environment:
      CUB_CLASSPATH: '/usr/share/java/confluent-security/schema-registry/*:/usr/share/java/schema-registry/*:/usr/share/java/schema-registry-plugins/*:/usr/share/java/cp-base-new/*'

      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1

      SCHEMA_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8085
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092

      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/kafka.truststore.jks
      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_ENABLED_PROTOCOLS: TLSv1.3
      SCHEMA_REGISTRY_KAFKASTORE_SSL_PROTOCOL: TLSv1.3

      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: SASL_SSL
      SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM: PLAIN
      SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG: |
        org.apache.kafka.common.security.plain.PlainLoginModule required \
        username="schema_registry" \
        password="${KAFKA_USER_SCHEMA_REGISTRY_PASSWORD}";

      SCHEMA_REGISTRY_EXPORTER_CONFIG_TOPIC: _csr_exporter_configs
      SCHEMA_REGISTRY_EXPORTER_STATE_TOPIC: _csr_exporter_states
      SCHEMA_REGISTRY_METADATA_ENCODER_TOPIC: _csr_schema_encoders
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _csr_schemas
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3

      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: full_transitive
    depends_on:
        kafka1:
            condition: service_healthy
        kafka2:
            condition: service_healthy
        kafka3:
            condition: service_healthy
    healthcheck:
      test: curl --fail --silent --insecure http://schema-registry-1:8085/subjects --output /dev/null || exit 1
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 10s
    restart: "no"
    mem_limit: 1GB
    cpus: 2

  prometheus:
    image: 'prom/prometheus:v3.5.0'
    container_name: prometheus
    hostname: prometheus
    networks:
      - monitoring_network
      - frontend_network
    user: "0:0" # Run as root to ensure access to mounted config files
    volumes:
      - prometheus-data:/var/lib/prometheus/data
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/var/lib/prometheus/data'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    depends_on:
      pg-exporter:
        condition: service_healthy
      scylla-node1:
        condition: service_healthy
      scylla-node2:
        condition: service_healthy
      scylla-node3:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 2GB
    cpus: 4

  grafana:
    image: 'grafana/grafana:12.3.0-17750354453'
    container_name: grafana
    hostname: grafana
    networks:
      - monitoring_network
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./grafana/provisioning/:/etc/grafana/provisioning/:ro
      - ./grafana/dashboards/:/etc/grafana/dashboards/:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-s", "--fail", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1GB
    cpus: 2

  chat-app-1:
    build:
      context: ../../app
      dockerfile: Dockerfile.dev
    container_name: chat-app-1
    hostname: chat-app-1
    networks:
      - frontend_network
      - redis_backend_network
    volumes:
      - ../../app:/app
      - ../../config:/app/config:ro
      - ./kafka/secrets/ca.crt:/app/certs/kafka/ca/public.crt:ro
      - es-certs:/app/certs/es:ro
      - ./cert-generator/output/ca/root.crt:/app/certs/root/ca/public.crt:ro
      - ./cert-generator/output/services/chat-app-1/public.crt:/app/certs/own/public.crt:ro
      - ./cert-generator/output/services/chat-app-1/private.key:/app/certs/own/private.key:ro
    environment:
      ENV: development
      BUILD_VERSION: ${CHAT_APP_VERSION}
      BUILD_COMMIT: ${CHAT_APP_COMMIT}
      BUILD_TIME: ${CHAT_APP_BUILD_TIME}
    depends_on:
      pgpool:
        condition: service_healthy
      scylla-node1:
        condition: service_healthy
      scylla-node2:
        condition: service_healthy
      scylla-node3:
        condition: service_healthy
      redis-node-1:
        condition: service_healthy
      redis-node-2:
        condition: service_healthy
      redis-node-3:
        condition: service_healthy
      redis-node-4:
        condition: service_healthy
      redis-node-5:
        condition: service_healthy
      redis-node-6:
        condition: service_healthy
      es-coordinating-1:
        condition: service_healthy
      filebeat:
        condition: service_healthy
      neo4j-node-1:
        condition: service_healthy
      kafka1:
        condition: service_healthy
      kafka2:
        condition: service_healthy
      kafka3:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    mem_limit: 2GB
    cpus: 4
    labels:
      "co.elastic.logs/enabled": true
      "co.elastic.logs/json.expand_keys": true
      "co.elastic.logs/json.add_error_key": true
      "co.elastic.logs/json.message_key": message
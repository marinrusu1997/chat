name: chatapp-cluster

networks:
  postgres_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.18.0.0/16
  scylla_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.19.0.0/16
  monitoring_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
  frontend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
  redis_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/16
  elasticsearch_backend_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.23.0.0/16

volumes:
  etcd-data:
  pg-data-1:
  pg-data-2:
  pg-data-3:
  scylla-data-1:
  scylla-data-2:
  scylla-data-3:
  redis-data-1:
  redis-data-2:
  redis-data-3:
  redis-data-4:
  redis-data-5:
  redis-data-6:
  es-master-data-1:
  es-hot-data-1:
  es-warm-data-1:
  es-certs: # Shared Elasticsearch SSL certificates
  es-kibana-data:
  prometheus-data:
  grafana-data:

services:
  etcd:
    image: gcr.io/etcd-development/etcd:v3.6.4
    container_name: etcd
    hostname: etcd
    networks:
      - postgres_backend_network
    volumes:
      - etcd-data:/var/log/etcd
    command: >
      /usr/local/bin/etcd
      --name etcd
      --data-dir /var/log/etcd
      --listen-client-urls http://0.0.0.0:2379
      --advertise-client-urls http://etcd:2379
      --listen-peer-urls http://0.0.0.0:2380
      --initial-advertise-peer-urls http://etcd:2380
      --initial-cluster etcd=http://etcd:2380
      --log-level info
      --logger zap
      --log-outputs stderr
    healthcheck:
      test: ["CMD", "/usr/local/bin/etcdctl", "--endpoints=http://etcd:2379", "endpoint", "health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 512M
    cpus: 0.5

  # A pool of identical PostgreSQL nodes managed by Patroni.
  pg-node-1:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-1
    hostname: pg-node-1
    networks:
      - postgres_backend_network
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-1:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
    environment:
      - PATRONI_NAME=pg-node-1
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-1:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-1:5432
      - PGPOOL_BACKEND_NODE_ID=0
    depends_on:
      etcd:
        condition: service_healthy
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q"]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12

  pg-node-2:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-2
    hostname: pg-node-2
    networks:
      - postgres_backend_network
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-2:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
    environment:
      - PATRONI_NAME=pg-node-2
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-2:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-2:5432
      - PGPOOL_BACKEND_NODE_ID=1
    depends_on:
      etcd:
        condition: service_healthy
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: [ "CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q" ]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12

  pg-node-3:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    container_name: pg-node-3
    hostname: pg-node-3
    networks:
      - postgres_backend_network
    volumes:
      # Configuration
      - ./postgresql/conf/patroni.yml:/tmp/patroni.yml.template:ro
      # Scripts
      - ./postgresql/migrations:/etc/patroni/migrations:ro
      - ./postgresql/scripts/init.sql:/etc/patroni/init.sql:ro
      - ./postgresql/scripts/entrypoint.sh:/etc/patroni/entrypoint.sh:ro
      - ./postgresql/scripts/post_init.sh:/etc/patroni/post_init.sh:ro
      - ./postgresql/scripts/on_role_change.sh:/etc/patroni/on_role_change.sh
      - ./postgresql/scripts/on_start.sh:/etc/patroni/on_start.sh
      - ./postgresql/scripts/on_stop.sh:/etc/patroni/on_stop.sh
      - ./postgresql/scripts/pcp_config.sh:/etc/patroni/pcp_config.sh
      - ./postgresql/scripts/interpolate_patroni_config.sh:/etc/patroni/interpolate_patroni_config.sh:ro
      # Data directory
      - pg-data-3:/var/lib/postgresql/data
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./postgresql/secrets/passwords/postgresql/admin.pass:/run/secrets/db_user_admin.pass
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_user_chat_ro.pass
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_user_chat_rw.pass
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass
      - ./postgresql/secrets/passwords/postgresql/replicator.pass:/run/secrets/db_user_replicator.pass
      - ./postgresql/secrets/passwords/postgresql/rewinder.pass:/run/secrets/db_user_rewinder.pass
      - ./postgresql/secrets/passwords/patroni/admin.pass:/run/secrets/patroni_user_admin.pass
      - ./pgpool/secrets/pcppass.txt:/tmp/configs/pcppass
    environment:
      - PATRONI_NAME=pg-node-3
      - PATRONI_RESTAPI_CONNECT_ADDRESS=pg-node-3:8008
      - PATRONI_POSTGRESQL_CONNECT_ADDRESS=pg-node-3:5432
      - PGPOOL_BACKEND_NODE_ID=2
    depends_on:
      etcd:
        condition: service_healthy
    init: true
    entrypoint: [ "/etc/patroni/entrypoint.sh" ]
    stop_grace_period: 30s
    healthcheck:
      test: [ "CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_user_replicator.pass gosu postgres pg_isready -U replicator -d postgres -q" ]
      interval: 10s
      timeout: 5s
      retries: 60 # Allow up to 10 minutes for initial setup
    restart: "no"
    mem_limit: 6G
    cpus: 12

  pgpool:
    build:
      context: ./pgpool
      dockerfile: Dockerfile
    container_name: pgpool
    hostname: pgpool
    networks:
      - postgres_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./pgpool/conf/pcp.conf:/etc/pgpool2/pcp.conf
      - ./pgpool/conf/pgpool.conf:/tmp/pgpool.conf.template:ro
      - ./pgpool/conf/pool_hba.conf:/etc/pgpool2/pool_hba.conf:ro
      # SSL Certificates
      - ./pgpool/certs:/etc/pgpool2/certs
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./pgpool/secrets/pgpoolkey.txt:/var/lib/postgresql/.pgpoolkey
      - ./pgpool/secrets/pcppass.txt:/var/lib/postgresql/.pcppass
      - ./postgresql/secrets/passwords/postgresql/pgpool_health.pass:/run/secrets/db_user_pgpool_health.pass:ro
      - ./postgresql/secrets/passwords/postgresql/chat_ro.pass:/run/secrets/db_external_user_chat_ro.pass:ro
      - ./postgresql/secrets/passwords/postgresql/chat_rw.pass:/run/secrets/db_external_user_chat_rw.pass:ro
      # Scripts
      - ./pgpool/scripts/pre_init.sh:/etc/pgpool2/custom_pre_init.sh:ro
      - ./pgpool/scripts/follow_primary.sh:/etc/pgpool2/custom_follow_primary.sh:ro
      - ./pgpool/scripts/failover.sh:/etc/pgpool2/custom_failover.sh:ro
      - ./pgpool/scripts/failback.sh:/etc/pgpool2/custom_failback.sh:ro
    depends_on:
      pg-node-1:
        condition: service_healthy
      pg-node-2:
        condition: service_healthy
      pg-node-3:
        condition: service_healthy
    command: >
      sh -c "
        chown postgres:postgres /var/lib/postgresql/.pgpoolkey && chmod 400 /var/lib/postgresql/.pgpoolkey &&
        chown postgres:postgres /etc/pgpool2/pcp.conf && chmod 400 /etc/pgpool2/pcp.conf &&
        chown -R postgres:postgres /etc/pgpool2/certs && chmod -R 400 /etc/pgpool2/certs && chmod 500 /etc/pgpool2/certs &&
        bash /etc/pgpool2/custom_pre_init.sh &&
        chown postgres:postgres /var/lib/postgresql/.pcppass && chmod 400 /var/lib/postgresql/.pcppass && 
        rm -f /tmp/pgpool.pid /tmp/.s.PGSQL.* && 
        exec gosu postgres /usr/sbin/pgpool -n -f /etc/pgpool2/pgpool.conf
      "
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD_FILE=/run/secrets/db_external_user_chat_ro.pass gosu postgres pg_isready -h pgpool -p 9999 -U chat_ro -d chat_db -q"]
      interval: 60s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1G
    cpus: 4

  pg-exporter:
    build:
      context: ./pg-exporter
      dockerfile: Dockerfile
    container_name: pg-exporter
    hostname: pg-exporter
    networks:
      - postgres_backend_network
      - monitoring_network
    volumes:
      - ./pg-exporter/postgres_exporter.yml:/tmp/postgres_exporter.yml.template:ro
      - ./pg-exporter/entrypoint.sh:/tmp/entrypoint.sh:ro
      - ./postgresql/secrets/passwords/postgresql/pgmonitor.pass:/run/secrets/db_user_pgmonitor.pass:ro
    entrypoint: [ '/tmp/entrypoint.sh' ]
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:9187/" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    depends_on:
      pgpool:
        condition: service_healthy
    mem_limit: 200MB
    cpus: 1

  scylla-node1:
    image: scylladb/scylla:2025.3
    container_name: scylla-node1
    hostname: scylla-node1
    networks:
      scylla_backend_network:
        aliases:
          - scylla-node1-internal # Give this node a specific name on the backend network
      frontend_network: # <-- Now you can safely add the frontend network
    volumes:
      - ./scylla/config/rack1.properties:/etc/scylla/cassandra-rackdc.properties:ro
      - scylla-data-1:/var/lib/scylla
    security_opt:
      - "seccomp:unconfined"
    command: >
      --seeds=scylla-node1-internal
      --listen-address=scylla-node1-internal
      --rpc-address=scylla-node1
      --native-shard-aware-transport-port=19042
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
    healthcheck:
      test: [ "CMD-SHELL", "nodetool status && sleep 10" ]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN

  scylla-node2:
    image: scylladb/scylla:2025.3
    container_name: scylla-node2
    hostname: scylla-node2
    networks:
      scylla_backend_network:
        aliases:
          - scylla-node2-internal
      frontend_network:
    volumes:
      - ./scylla/config/rack2.properties:/etc/scylla/cassandra-rackdc.properties:ro
      - scylla-data-2:/var/lib/scylla
    security_opt:
      - "seccomp:unconfined"
    command: >
      --seeds=scylla-node1-internal
      --listen-address=scylla-node2-internal
      --rpc-address=scylla-node2
      --native-shard-aware-transport-port=19042
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
    healthcheck:
      test: [ "CMD-SHELL", "nodetool status && sleep 5" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    depends_on:
      scylla-node1:
        condition: service_healthy
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN

  scylla-node3:
    image: scylladb/scylla:2025.3
    container_name: scylla-node3
    hostname: scylla-node3
    networks:
      scylla_backend_network:
        aliases:
          - scylla-node3-internal
      frontend_network:
    volumes:
      - ./scylla/config/rack3.properties:/etc/scylla/cassandra-rackdc.properties:ro
      - scylla-data-3:/var/lib/scylla
    security_opt:
      - "seccomp:unconfined"
    command: >
      --seeds=scylla-node1-internal
      --listen-address=scylla-node3-internal
      --rpc-address=scylla-node3
      --native-shard-aware-transport-port=19042
      --smp 2 
      --memory 2200M 
      --developer-mode 1 
      --reactor-backend=io_uring 
      --cluster-name=ScyllaChatCluster 
      --endpoint-snitch=GossipingPropertyFileSnitch 
      --write-request-timeout-in-ms=3000 
      --read-request-timeout-in-ms=7000 
      --range-request-timeout-in-ms=12000 
      --num-tokens=256 
      --auto-bootstrap=1
      --commitlog-sync=batch
      --commitlog-sync-batch-window-in-ms=2
      --compaction-large-partition-warning-threshold-mb=1000
      --tombstone-warn-threshold=5000
      --restrict-twcs-without-default-ttl=true
      --authenticator=PasswordAuthenticator
      --authorizer=CassandraAuthorizer
      --enable-user-defined-functions=1
      --experimental-features=alternator-streams
      --experimental-features=broadcast-tables
      --experimental-features=keyspace-storage-options
      --experimental-features=udf
      --experimental-features=views-with-tablets
    healthcheck:
      test: [ "CMD-SHELL", "nodetool status && sleep 5" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    depends_on:
      scylla-node2:
        condition: service_healthy
    mem_limit: 4G
    cpus: 2
    cap_add:
      - SYS_ADMIN

  scylla-maintainer:
    image: scylladb/scylla:2025.3
    container_name: scylla-maintainer
    networks:
      - frontend_network
    volumes:
      - ./scylla/scripts/init.cql:/tmp/init.cql:ro
      - ./scylla/scripts/init.sh:/tmp/init.sh:ro
    env_file:
      - ./scylla/secrets/.env.secrets
    entrypoint: sh /tmp/init.sh
    restart: no
    depends_on:
      scylla-node1:
        condition: service_healthy
      scylla-node2:
        condition: service_healthy
      scylla-node3:
        condition: service_healthy

  redis-node-1:
    image: redis:8.2.1
    container_name: redis-node-1
    hostname: redis-node-1
    networks:
      - redis_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Scripts
      - ./redis/scripts/entrypoint.sh:/usr/local/etc/redis/scripts/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/usr/local/etc/redis/scripts/logger.sh:ro
      - ./redis/scripts/healthcheck.sh:/usr/local/etc/redis/scripts/healthcheck.sh:ro
      # Data
      - redis-data-1:/data
    entrypoint: /usr/local/etc/redis/scripts/entrypoint.sh
    command: >
      /usr/local/etc/redis/redis.conf 
      --port 6379 
      --cluster-announce-hostname redis-node-1
      --cluster-announce-human-nodename redis-node-1
      --cluster-announce-ip redis-node-1
      --cluster-announce-port 6379
      --cluster-announce-bus-port 16379
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-2:
    image: redis:8.2.1
    container_name: redis-node-2
    hostname: redis-node-2
    networks:
      - redis_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Scripts
      - ./redis/scripts/entrypoint.sh:/usr/local/etc/redis/scripts/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/usr/local/etc/redis/scripts/logger.sh:ro
      - ./redis/scripts/healthcheck.sh:/usr/local/etc/redis/scripts/healthcheck.sh:ro
      # Data
      - redis-data-2:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --port 6379 
      --cluster-announce-hostname redis-node-2
      --cluster-announce-human-nodename redis-node-2
      --cluster-announce-ip redis-node-2
      --cluster-announce-port 6379
      --cluster-announce-bus-port 16379
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-3:
    image: redis:8.2.1
    container_name: redis-node-3
    hostname: redis-node-3
    networks:
      - redis_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Scripts
      - ./redis/scripts/entrypoint.sh:/usr/local/etc/redis/scripts/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/usr/local/etc/redis/scripts/logger.sh:ro
      - ./redis/scripts/healthcheck.sh:/usr/local/etc/redis/scripts/healthcheck.sh:ro
      # Data
      - redis-data-3:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --port 6379 
      --cluster-announce-hostname redis-node-3
      --cluster-announce-human-nodename redis-node-3
      --cluster-announce-ip redis-node-3
      --cluster-announce-port 6379
      --cluster-announce-bus-port 16379
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-4:
    image: redis:8.2.1
    container_name: redis-node-4
    hostname: redis-node-4
    networks:
      - redis_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Scripts
      - ./redis/scripts/entrypoint.sh:/usr/local/etc/redis/scripts/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/usr/local/etc/redis/scripts/logger.sh:ro
      - ./redis/scripts/healthcheck.sh:/usr/local/etc/redis/scripts/healthcheck.sh:ro
      # Data
      - redis-data-4:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --port 6379 
      --cluster-announce-hostname redis-node-4
      --cluster-announce-human-nodename redis-node-4
      --cluster-announce-ip redis-node-4
      --cluster-announce-port 6379
      --cluster-announce-bus-port 16379
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-5:
    image: redis:8.2.1
    container_name: redis-node-5
    hostname: redis-node-5
    networks:
      - redis_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Scripts
      - ./redis/scripts/entrypoint.sh:/usr/local/etc/redis/scripts/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/usr/local/etc/redis/scripts/logger.sh:ro
      - ./redis/scripts/healthcheck.sh:/usr/local/etc/redis/scripts/healthcheck.sh:ro
      # Data
      - redis-data-5:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --port 6379 
      --cluster-announce-hostname redis-node-5
      --cluster-announce-human-nodename redis-node-5
      --cluster-announce-ip redis-node-5
      --cluster-announce-port 6379
      --cluster-announce-bus-port 16379
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  redis-node-6:
    image: redis:8.2.1
    container_name: redis-node-6
    hostname: redis-node-6
    networks:
      - redis_backend_network
      - frontend_network
    volumes:
      # Configuration
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - ./redis/conf/users.acl:/usr/local/etc/redis/users.acl
      # Secrets (in a real deployment, we will use Docker secrets)
      - ./redis/secrets/default.pass:/usr/local/etc/redis/secrets/default.pass:ro
      - ./redis/secrets/replicator.pass:/usr/local/etc/redis/secrets/replicator.pass:ro
      # Scripts
      - ./redis/scripts/entrypoint.sh:/usr/local/etc/redis/scripts/entrypoint.sh:ro
      - ./redis/scripts/logger.sh:/usr/local/etc/redis/scripts/logger.sh:ro
      - ./redis/scripts/healthcheck.sh:/usr/local/etc/redis/scripts/healthcheck.sh:ro
      # Data
      - redis-data-6:/data
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --port 6379 
      --cluster-announce-hostname redis-node-6
      --cluster-announce-human-nodename redis-node-6
      --cluster-announce-ip redis-node-6
      --cluster-announce-port 6379
      --cluster-announce-bus-port 16379
    healthcheck:
      test: ["CMD", "bash", "/usr/local/etc/redis/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: "no"
    mem_limit: 1300M
    cpus: 4
    sysctls:
      - net.core.somaxconn=1024

  es-setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-setup
    hostname: es-setup
    networks:
      - elasticsearch_backend_network
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - ./elasticsearch/certs/instances.yml:/usr/share/elasticsearch/config/certs/instances.yml
      - ./elasticsearch/scripts/init.sh:/tmp/scripts/init.sh:ro
      - ./redis/scripts/logger.sh:/tmp/scripts/logger.sh:ro
      - ./elasticsearch/schemas/chat-messages/settings.json:/tmp/schemas/chat-messages/settings.json:ro
      - ./elasticsearch/schemas/chat-messages/mappings.json:/tmp/schemas/chat-messages/mappings.json:ro
      - ./elasticsearch/schemas/chat-messages/template.json:/tmp/schemas/chat-messages/template.json:ro
    environment:
      - ELASTICSEARCH_ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - ELASTICSEARCH_KIBANA_SYSTEM_USERNAME=${ELASTICSEARCH_KIBANA_SYSTEM_USERNAME}
      - ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD=${ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD}
      - ELASTICSEARCH_KIBANA_ADMIN_USERNAME=${ELASTICSEARCH_KIBANA_ADMIN_USERNAME}
      - ELASTICSEARCH_KIBANA_ADMIN_PASSWORD=${ELASTICSEARCH_KIBANA_ADMIN_PASSWORD}
      - ELASTICSEARCH_CHAT_APP_USERNAME=${ELASTICSEARCH_CHAT_APP_USERNAME}
      - ELASTICSEARCH_CHAT_APP_PASSWORD=${ELASTICSEARCH_CHAT_APP_PASSWORD}
    user: "0"
    entrypoint: [ "/bin/bash", "/tmp/scripts/init.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "[ -f config/certs/es-master-1/es-master-1.crt ]" ]
      interval: 10s
      timeout: 5s
      retries: 120
    restart: no
    mem_limit: 200M
    cpus: 1

  es-master-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-master-1
    hostname: es-master-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.11
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - es-master-data-1:/usr/share/elasticsearch/data
    environment:
      - node.name=es-master-1
      - node.roles=master
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=""
      - cluster.initial_master_nodes=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true # Rule: Disable swapping for performance
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" # Rule: Set -Xms and -Xmx equal. Use small heap for master nodes.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-master-1/es-master-1.key
      - xpack.security.http.ssl.certificate=certs/es-master-1/es-master-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-master-1/es-master-1.key
      - xpack.security.transport.ssl.certificate=certs/es-master-1/es-master-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL","curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-master-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 2GB
    cpus: 1
    depends_on:
      es-setup: { condition: service_healthy }

  es-data-hot-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-data-hot-1
    hostname: es-data-hot-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.21
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - es-hot-data-1:/usr/share/elasticsearch/data
    environment:
      - node.name=es-data-hot-1
      - node.roles=data
      - node.attr.data_tier=hot
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g" # IMPORTANT: Set this to <= 50% of the host RAM, but do not exceed ~31GB.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-data-hot-1/es-data-hot-1.key
      - xpack.security.http.ssl.certificate=certs/es-data-hot-1/es-data-hot-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-data-hot-1/es-data-hot-1.key
      - xpack.security.transport.ssl.certificate=certs/es-data-hot-1/es-data-hot-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-data-hot-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 6GB
    cpus: 6
    depends_on:
      es-setup: { condition: service_healthy }

  es-data-warm-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-data-warm-1
    hostname: es-data-warm-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.22
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
      - es-warm-data-1:/usr/share/elasticsearch/data
    environment:
      - node.name=es-data-warm-1
      - node.roles=data
      - node.attr.data_tier=warm
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g" # IMPORTANT: Set this to <= 50% of the host RAM, but do not exceed ~31GB.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-data-warm-1/es-data-warm-1.key
      - xpack.security.http.ssl.certificate=certs/es-data-warm-1/es-data-warm-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-data-warm-1/es-data-warm-1.key
      - xpack.security.transport.ssl.certificate=certs/es-data-warm-1/es-data-warm-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-data-warm-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 6GB
    cpus: 6
    depends_on:
      es-setup: { condition: service_healthy }

  es-coordinating-1:
    image: docker.elastic.co/elasticsearch/elasticsearch-wolfi:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-coordinating-1
    hostname: es-coordinating-1
    networks:
      elasticsearch_backend_network:
        ipv4_address: 172.23.0.31
    volumes:
      - es-certs:/usr/share/elasticsearch/config/certs
    environment:
      - node.name=es-coordinating-1
      - node.roles="" # Empty role makes it coordinating-only
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - discovery.seed_hosts=es-master-1
      - ELASTIC_PASSWORD=${ELASTICSEARCH_ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms800m -Xmx800m" # Moderate heap for query aggregation.
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es-coordinating-1/es-coordinating-1.key
      - xpack.security.http.ssl.certificate=certs/es-coordinating-1/es-coordinating-1.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es-coordinating-1/es-coordinating-1.key
      - xpack.security.transport.ssl.certificate=certs/es-coordinating-1/es-coordinating-1.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=full
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u elastic:${ELASTICSEARCH_ELASTIC_PASSWORD} https://es-coordinating-1:9200/_cluster/health | grep -q -E '\"status\":\"(yellow|green)\"'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: "no"
    mem_limit: 4GB
    cpus: 4
    depends_on:
      es-setup: { condition: service_healthy }

  es-kibana:
    image: docker.elastic.co/kibana/kibana:${ELASTICSEARCH_STACK_VERSION}
    container_name: es-kibana
    hostname: es-kibana
    ports:
      - "5781:5601"
    networks:
      - elasticsearch_backend_network
    volumes:
      - es-kibana-data:/usr/share/kibana/data
      - es-certs:/usr/share/kibana/config/certs
    environment:
      - SERVERNAME=es-kibana
      - ELASTICSEARCH_HOSTS=https://es-coordinating-1:9200
      - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_KIBANA_SYSTEM_USERNAME}
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_KIBANA_SYSTEM_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=/usr/share/kibana/config/certs/ca/ca.crt
    restart: "no"
    mem_limit: 1GB
    cpus: 2
    depends_on:
      es-setup: { condition: service_healthy }

  prometheus:
    image: 'prom/prometheus:v3.5.0'
    container_name: prometheus
    hostname: prometheus
    networks:
      - monitoring_network
      - frontend_network
    user: "0:0" # Run as root to ensure access to mounted config files
    volumes:
      - prometheus-data:/var/lib/prometheus/data
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/var/lib/prometheus/data'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    depends_on:
      pg-exporter:
        condition: service_healthy
      scylla-node1:
        condition: service_healthy
      scylla-node2:
        condition: service_healthy
      scylla-node3:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: "no"
    mem_limit: 2GB
    cpus: 4

  grafana:
    image: 'grafana/grafana:12.3.0-17750354453'
    container_name: grafana
    hostname: grafana
    networks:
      - monitoring_network
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./grafana/provisioning/:/etc/grafana/provisioning/:ro
      - ./grafana/dashboards/:/etc/grafana/dashboards/:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-s", "--fail", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: "no"
    mem_limit: 1GB
    cpus: 2

  chat-app:
    build:
      context: ../../app
      dockerfile: Dockerfile.dev
    container_name: chat-app
    hostname: chat-app
    networks:
      - frontend_network
    volumes:
      - ../../app:/app
    environment:
      - ENV=development
    depends_on:
      pgpool:
        condition: service_healthy
      scylla-node1:
        condition: service_healthy
      scylla-node2:
        condition: service_healthy
      scylla-node3:
        condition: service_healthy
      redis-node-1:
        condition: service_healthy
      redis-node-2:
        condition: service_healthy
      redis-node-3:
        condition: service_healthy
      redis-node-4:
        condition: service_healthy
      redis-node-5:
        condition: service_healthy
      redis-node-6:
        condition: service_healthy
      es-coordinating-1:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    mem_limit: 2GB
    cpus: 4